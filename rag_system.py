"""
RAG (Retrieval-Augmented Generation) ì‹œìŠ¤í…œ
Pinecone ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì™€ OpenAIë¥¼ í™œìš©í•œ ì§€ëŠ¥í˜• ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ
"""

import json
import hashlib
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime, timezone, timedelta
import asyncio
import re
import os
import time

from config import config
from logger import get_logger, log_chatbot_interaction, monitor_performance

try:
    import pinecone
    from pinecone import Pinecone, ServerlessSpec
    PINECONE_AVAILABLE = True
except ImportError:
    PINECONE_AVAILABLE = False
    print("ê²½ê³ : pinecone-clientê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. RAG ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤.")

try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False
    print("ê²½ê³ : sentence-transformersê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„ë² ë”© ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤.")

try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    print("ê²½ê³ : openaiê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í…ìŠ¤íŠ¸ ìƒì„± ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤.")

logger = get_logger(__name__)

class EmbeddingManager:
    """í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± ë° ê´€ë¦¬"""
    
    def __init__(self):
        self.model = None
        self.model_name = "distiluse-base-multilingual-cased"
        self.embedding_dimension = None
        self._initialize_model()
    
    def _initialize_model(self):
        """ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”"""
        if not SENTENCE_TRANSFORMERS_AVAILABLE:
            logger.warning("sentence-transformersë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        try:
            self.model = SentenceTransformer(self.model_name)
            
            # ì‹¤ì œ ì„ë² ë”© ì°¨ì› í™•ì¸
            test_embedding = self.model.encode(["test"])
            self.embedding_dimension = len(test_embedding[0])
            
            logger.info(f"ì„ë² ë”© ëª¨ë¸ '{self.model_name}' ë¡œë“œ ì™„ë£Œ")
            logger.info(f"ì„ë² ë”© ì°¨ì›: {self.embedding_dimension}")
            
        except Exception as e:
            logger.error(f"ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def get_embedding_dimension(self) -> int:
        """ì„ë² ë”© ì°¨ì› ë°˜í™˜"""
        return self.embedding_dimension or 512  # ê¸°ë³¸ê°’ 512
    
    @monitor_performance
    def create_embedding(self, text: str) -> List[float]:
        """í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜"""
        if not self.model:
            raise ValueError("ì„ë² ë”© ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        try:
            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
            cleaned_text = self._preprocess_text(text)
            # ì„ë² ë”© ìƒì„±
            embedding = self.model.encode(cleaned_text).tolist()
            return embedding
        except Exception as e:
            logger.error(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            raise
    
    def create_batch_embeddings(self, texts: List[str]) -> List[List[float]]:
        """ì—¬ëŸ¬ í…ìŠ¤íŠ¸ë¥¼ í•œ ë²ˆì— ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜"""
        if not self.model:
            raise ValueError("ì„ë² ë”© ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        try:
            cleaned_texts = [self._preprocess_text(text) for text in texts]
            embeddings = self.model.encode(cleaned_texts).tolist()
            return embeddings
        except Exception as e:
            logger.error(f"ë°°ì¹˜ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            raise
    
    def _preprocess_text(self, text: str) -> str:
        """ì„ë² ë”©ì„ ìœ„í•œ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        if not text:
            return ""
        
        # ê¸°ë³¸ì ì¸ ì •ì œ
        text = text.strip()
        # ê¸´ í…ìŠ¤íŠ¸ ìë¥´ê¸° (ëª¨ë¸ ì œí•œ ê³ ë ¤)
        max_length = 512
        if len(text) > max_length:
            text = text[:max_length]
        
        return text

class PineconeManager:
    """Pinecone ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬"""
    
    def __init__(self, embedding_dimension: Optional[int] = None):
        self.client = None
        self.index = None
        self.embedding_dimension = embedding_dimension
        self._initialize_pinecone()
    
    def _initialize_pinecone(self):
        """Pinecone ì´ˆê¸°í™”"""
        if not PINECONE_AVAILABLE:
            logger.warning("Pineconeì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        if not config.PINECONE_API_KEY:
            logger.warning("PINECONE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
        
        try:
            logger.info(f"Pinecone ì´ˆê¸°í™” ì‹œì‘...")
            logger.info(f"API í‚¤: {config.PINECONE_API_KEY[:20]}...")
            logger.info(f"ì¸ë±ìŠ¤ëª…: {config.PINECONE_INDEX_NAME}")
            logger.info(f"ì„ë² ë”© ì°¨ì›: {self.embedding_dimension}")
            
            # Pinecone í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
            self.client = Pinecone(api_key=config.PINECONE_API_KEY)
            logger.info("Pinecone í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
            
            # ì¸ë±ìŠ¤ ì¡´ì¬ í™•ì¸ ë° ìƒì„±
            self._ensure_index_exists()
            
            # ì¸ë±ìŠ¤ ì—°ê²°
            self.index = self.client.Index(config.PINECONE_INDEX_NAME)
            logger.info(f"Pinecone ì¸ë±ìŠ¤ '{config.PINECONE_INDEX_NAME}' ì—°ê²° ì™„ë£Œ")
            
            # ì—°ê²° í…ŒìŠ¤íŠ¸
            stats = self.index.describe_index_stats()
            logger.info(f"ì¸ë±ìŠ¤ í†µê³„: {stats}")
            
        except Exception as e:
            logger.error(f"Pinecone ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {type(e).__name__}: {str(e)}")
            raise
    
    def _ensure_index_exists(self):
        """ì¸ë±ìŠ¤ê°€ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒì„±"""
        try:
            # ê¸°ì¡´ ì¸ë±ìŠ¤ ëª©ë¡ í™•ì¸
            existing_indexes = [index_info["name"] for index_info in self.client.list_indexes()]
            
            if config.PINECONE_INDEX_NAME not in existing_indexes:
                logger.info(f"ì¸ë±ìŠ¤ '{config.PINECONE_INDEX_NAME}' ìƒì„± ì¤‘...")
                
                # ì‹¤ì œ ì„ë² ë”© ì°¨ì› ì‚¬ìš©
                dimension = self.embedding_dimension or config.EMBEDDING_DIMENSION
                
                self.client.create_index(
                    name=config.PINECONE_INDEX_NAME,
                    dimension=dimension,
                    metric="cosine",
                    spec=ServerlessSpec(
                        cloud="aws",
                        region="us-east-1"
                    )
                )
                logger.info(f"ì¸ë±ìŠ¤ '{config.PINECONE_INDEX_NAME}' ìƒì„± ì™„ë£Œ (ì°¨ì›: {dimension})")
            else:
                # ê¸°ì¡´ ì¸ë±ìŠ¤ì˜ ì°¨ì› í™•ì¸
                index_stats = self.client.describe_index(config.PINECONE_INDEX_NAME)
                existing_dimension = index_stats.dimension
                expected_dimension = self.embedding_dimension or config.EMBEDDING_DIMENSION
                
                if existing_dimension != expected_dimension:
                    logger.warning(f"ê¸°ì¡´ ì¸ë±ìŠ¤ ì°¨ì›({existing_dimension})ê³¼ ì„ë² ë”© ëª¨ë¸ ì°¨ì›({expected_dimension})ì´ ë¶ˆì¼ì¹˜í•©ë‹ˆë‹¤.")
                    logger.warning("ê¸°ì¡´ ì¸ë±ìŠ¤ë¥¼ ì‚­ì œí•˜ê³  ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤...")
                    
                    # ê¸°ì¡´ ì¸ë±ìŠ¤ ì‚­ì œ
                    self.client.delete_index(config.PINECONE_INDEX_NAME)
                    
                    # ìƒˆ ì¸ë±ìŠ¤ ìƒì„±
                    self.client.create_index(
                        name=config.PINECONE_INDEX_NAME,
                        dimension=expected_dimension,
                        metric="cosine",
                        spec=ServerlessSpec(
                            cloud="aws",
                            region="us-east-1"
                        )
                    )
                    logger.info(f"ìƒˆ ì¸ë±ìŠ¤ '{config.PINECONE_INDEX_NAME}' ìƒì„± ì™„ë£Œ (ì°¨ì›: {expected_dimension})")
                else:
                    logger.info(f"ì¸ë±ìŠ¤ '{config.PINECONE_INDEX_NAME}' ì´ë¯¸ ì¡´ì¬ (ì°¨ì›: {existing_dimension})")
                
        except Exception as e:
            logger.error(f"ì¸ë±ìŠ¤ í™•ì¸/ìƒì„± ì‹¤íŒ¨: {e}")
            raise
    
    @monitor_performance
    def upsert_vectors(self, vectors: List[Dict[str, Any]]) -> bool:
        """ë²¡í„° ë°ì´í„° ì—…ì„œíŠ¸"""
        if not self.index:
            logger.error("Pinecone ì¸ë±ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return False
        
        try:
            # ë°°ì¹˜ í¬ê¸°ë¡œ ë¶„í• í•˜ì—¬ ì—…ì„œíŠ¸
            batch_size = 100
            for i in range(0, len(vectors), batch_size):
                batch = vectors[i:i + batch_size]
                self.index.upsert(vectors=batch)
            
            logger.info(f"{len(vectors)}ê°œ ë²¡í„° ì—…ì„œíŠ¸ ì™„ë£Œ")
            return True
            
        except Exception as e:
            logger.error(f"ë²¡í„° ì—…ì„œíŠ¸ ì‹¤íŒ¨: {e}")
            return False
    
    @monitor_performance
    def search_similar(self, query_vector: List[float], top_k: int = 30, filter_dict: Optional[Dict] = None) -> List[Dict[str, Any]]:
        """ìœ ì‚¬í•œ ë²¡í„° ê²€ìƒ‰ (ì‹ ì²­ ê°€ëŠ¥í•œ ì§€ì›ì‚¬ì—… ìš°ì„ )"""
        if not self.index:
            logger.error("Pinecone ì¸ë±ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []
        
        try:
            # ì‹ ì²­ ê°€ëŠ¥í•œ ì§€ì›ì‚¬ì—…ì„ ìš°ì„ ì ìœ¼ë¡œ ì°¾ê¸° ìœ„í•´ ë” ë§ì€ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
            extended_top_k = min(top_k * 5, 100)  # ìµœëŒ€ 100ê°œê¹Œì§€ í™•ì¥
            
            query_response = self.index.query(
                vector=query_vector,
                top_k=extended_top_k,
                include_metadata=True,
                filter=filter_dict
            )
            
            results = []
            current_year = datetime.now().year
            
            for match in query_response.matches:
                metadata = match.metadata
                
                # ì‹ ì²­ ê¸°ê°„ ë¶„ì„
                application_period = metadata.get('application_period', '')
                deadline_status = self._analyze_deadline_status(application_period)
                is_current_year = self._is_current_year_announcement(application_period, current_year)
                
                result = {
                    "id": match.id,
                    "score": match.score,
                    "metadata": metadata,
                    "is_current_year": is_current_year,
                    "deadline_status": deadline_status,
                    "is_applicable": not deadline_status["is_expired"]  # ì‹ ì²­ ê°€ëŠ¥ ì—¬ë¶€
                }
                
                results.append(result)
            
            # ì‹ ì²­ ê°€ëŠ¥í•œ ì§€ì›ì‚¬ì—…ì„ ìš°ì„ ì ìœ¼ë¡œ ì •ë ¬
            # 1ìˆœìœ„: ì‹ ì²­ ê°€ëŠ¥ ì—¬ë¶€ (ë§ˆê°ë˜ì§€ ì•ŠìŒ)
            # 2ìˆœìœ„: í˜„ì¬ ì—°ë„ ì—¬ë¶€
            # 3ìˆœìœ„: ë§ˆê° ì„ë°•ë„ (ë§ˆê°ì´ ê°€ê¹Œìš¸ìˆ˜ë¡ ìš°ì„ )
            # 4ìˆœìœ„: ìœ ì‚¬ë„ ì ìˆ˜
            def sort_key(x):
                deadline_status = x["deadline_status"]
                urgency_score = 0
                
                if deadline_status["status"] == "today":
                    urgency_score = 1000  # ì˜¤ëŠ˜ ë§ˆê° - ìµœìš°ì„ 
                elif deadline_status["status"] == "urgent":
                    urgency_score = 500   # 3ì¼ ì´ë‚´ ë§ˆê°
                elif deadline_status["days_remaining"] is not None and deadline_status["days_remaining"] > 0:
                    # ë§ˆê°ì¼ì´ ê°€ê¹Œìš¸ìˆ˜ë¡ ë†’ì€ ì ìˆ˜ (ìµœëŒ€ 30ì¼ ê¸°ì¤€)
                    urgency_score = max(0, 100 - deadline_status["days_remaining"])
                
                return (
                    not x["is_applicable"],      # ì‹ ì²­ ë¶ˆê°€ëŠ¥í•œ ê²ƒì€ ë’¤ë¡œ
                    not x["is_current_year"],    # í˜„ì¬ ì—°ë„ê°€ ì•„ë‹Œ ê²ƒì€ ë’¤ë¡œ
                    -urgency_score,              # ê¸´ê¸‰ë„ê°€ ë†’ì€ ê²ƒì„ ì•ìœ¼ë¡œ
                    -x["score"]                  # ìœ ì‚¬ë„ê°€ ë†’ì€ ê²ƒì„ ì•ìœ¼ë¡œ
                )
            
            results.sort(key=sort_key)
            
            # ìš”ì²­ëœ ê°œìˆ˜ë§Œí¼ ë°˜í™˜
            final_results = results[:top_k]
            
            # í†µê³„ ì •ë³´ ë¡œê¹…
            applicable_count = sum(1 for r in final_results if r["is_applicable"])
            current_year_count = sum(1 for r in final_results if r["is_current_year"])
            urgent_count = sum(1 for r in final_results if r["deadline_status"]["is_urgent"])
            
            logger.info(f"ìœ ì‚¬ë„ ê²€ìƒ‰ ì™„ë£Œ: {len(final_results)}ê°œ ê²°ê³¼ "
                       f"(ì‹ ì²­ê°€ëŠ¥: {applicable_count}ê°œ, í˜„ì¬ì—°ë„: {current_year_count}ê°œ, ê¸´ê¸‰: {urgent_count}ê°œ)")
            
            return final_results
            
        except Exception as e:
            logger.error(f"ìœ ì‚¬ë„ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _analyze_deadline_status(self, application_period: str) -> Dict[str, Any]:
        """ì§€ì›ì‚¬ì—… ë§ˆê°ì¼ ë¶„ì„ (YYYYMMDD í˜•ì‹ í¬í•¨)"""
        try:
            kst = timezone(timedelta(hours=9))
            now = datetime.now(kst)
            
            # ì ‘ìˆ˜ê¸°ê°„ì—ì„œ ë§ˆê°ì¼ ì¶”ì¶œ ì‹œë„
            import re
            
            deadline_info = {
                "status": "unknown",
                "days_remaining": None,
                "deadline_date": None,
                "is_expired": False,
                "is_urgent": False
            }
            
            if not application_period:
                return deadline_info
            
            # YYYYMMDD ~ YYYYMMDD í˜•ì‹ ìš°ì„  ì²˜ë¦¬
            yyyymmdd_pattern = r'(\d{8})\s*~\s*(\d{8})'
            yyyymmdd_match = re.search(yyyymmdd_pattern, application_period)
            
            if yyyymmdd_match:
                try:
                    end_date_str = yyyymmdd_match.group(2)  # ë§ˆê°ì¼
                    year = int(end_date_str[:4])
                    month = int(end_date_str[4:6])
                    day = int(end_date_str[6:8])
                    
                    deadline = datetime(year, month, day, 23, 59, 59, tzinfo=kst)
                    days_diff = (deadline - now).days
                    
                    deadline_info.update({
                        "deadline_date": deadline.strftime("%Y-%m-%d"),
                        "days_remaining": days_diff,
                        "is_expired": days_diff < 0,
                        "is_urgent": 0 <= days_diff <= 3
                    })
                    
                    if days_diff < 0:
                        deadline_info["status"] = "expired"
                    elif days_diff == 0:
                        deadline_info["status"] = "today"
                    elif days_diff <= 3:
                        deadline_info["status"] = "urgent"
                    elif days_diff <= 7:
                        deadline_info["status"] = "soon"
                    else:
                        deadline_info["status"] = "active"
                        
                    return deadline_info
                    
                except ValueError:
                    pass
            
            # YYYY.MM.DD í˜•ì‹ ì²˜ë¦¬
            dot_pattern = r'(\d{4})\.(\d{1,2})\.(\d{1,2})\s*~\s*(\d{4})\.(\d{1,2})\.(\d{1,2})'
            dot_match = re.search(dot_pattern, application_period)
            
            if dot_match:
                try:
                    year = int(dot_match.group(4))
                    month = int(dot_match.group(5))
                    day = int(dot_match.group(6))
                    
                    deadline = datetime(year, month, day, 23, 59, 59, tzinfo=kst)
                    days_diff = (deadline - now).days
                    
                    deadline_info.update({
                        "deadline_date": deadline.strftime("%Y-%m-%d"),
                        "days_remaining": days_diff,
                        "is_expired": days_diff < 0,
                        "is_urgent": 0 <= days_diff <= 3
                    })
                    
                    if days_diff < 0:
                        deadline_info["status"] = "expired"
                    elif days_diff == 0:
                        deadline_info["status"] = "today"
                    elif days_diff <= 3:
                        deadline_info["status"] = "urgent"
                    elif days_diff <= 7:
                        deadline_info["status"] = "soon"
                    else:
                        deadline_info["status"] = "active"
                        
                    return deadline_info
                    
                except ValueError:
                    pass
            
            # ê¸°íƒ€ í˜•ì‹ë“¤ë„ ì‹œë„í•  ìˆ˜ ìˆì§€ë§Œ, ê¸°ë³¸ê°’ ë°˜í™˜
            return deadline_info
            
        except Exception as e:
            logger.error(f"ë§ˆê°ì¼ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                "status": "unknown",
                "days_remaining": None,
                "deadline_date": None,
                "is_expired": False,
                "is_urgent": False
            }
    
    def _is_current_year_announcement(self, application_period: str, current_year: int) -> bool:
        """ì ‘ìˆ˜ê¸°ê°„ì—ì„œ í˜„ì¬ ì—°ë„ ì§€ì›ì‚¬ì—…ì¸ì§€ í™•ì¸"""
        if not application_period:
            return False
        
        try:
            import re
            # YYYYMMDD í˜•ì‹ì—ì„œ ì—°ë„ ì¶”ì¶œ
            year_matches = re.findall(r'(\d{4})', application_period)
            if year_matches:
                # ê°€ì¥ ìµœê·¼ ì—°ë„ í™•ì¸
                years = [int(year) for year in year_matches]
                latest_year = max(years)
                return latest_year >= current_year
            return False
        except Exception:
            return False
    
    def delete_vectors(self, ids: List[str]) -> bool:
        """íŠ¹ì • ë²¡í„°ë“¤ ì‚­ì œ"""
        if not self.index:
            logger.error("Pinecone ì¸ë±ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return False
        
        try:
            self.index.delete(ids=ids)
            logger.info(f"{len(ids)}ê°œ ë²¡í„° ì‚­ì œ ì™„ë£Œ")
            return True
        except Exception as e:
            logger.error(f"ë²¡í„° ì‚­ì œ ì‹¤íŒ¨: {e}")
            return False
    
    def get_index_stats(self) -> Dict[str, Any]:
        """ì¸ë±ìŠ¤ í†µê³„ ì •ë³´ ë°˜í™˜"""
        if not self.index:
            return {}
        
        try:
            stats = self.index.describe_index_stats()
            return {
                "total_vector_count": stats.total_vector_count,
                "dimension": stats.dimension,
                "index_fullness": stats.index_fullness
            }
        except Exception as e:
            logger.error(f"ì¸ë±ìŠ¤ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}

class RAGChatbot:
    """RAG ê¸°ë°˜ ì±—ë´‡ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.embedding_manager = EmbeddingManager()
        
        # ì„ë² ë”© ì°¨ì›ì„ ê°€ì ¸ì™€ì„œ PineconeManagerì— ì „ë‹¬
        embedding_dimension = self.embedding_manager.get_embedding_dimension()
        self.pinecone_manager = PineconeManager(embedding_dimension=embedding_dimension)
        
        self.openai_client = None
        self.chat_history = []
        self.conversation_memory = []  # ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ë¥¼ ìœ„í•œ ë©”ëª¨ë¦¬
        self.max_memory_turns = 5  # ìµœëŒ€ 5í„´ì˜ ëŒ€í™” ê¸°ì–µ
        self._initialize_openai()
    
    def _get_current_time_info(self) -> Dict[str, str]:
        """í˜„ì¬ í•œêµ­ ì‹œê°„ ì •ë³´ ë°˜í™˜"""
        # í•œêµ­ ì‹œê°„ëŒ€ (UTC+9)
        kst = timezone(timedelta(hours=9))
        now = datetime.now(kst)
        
        return {
            "current_date": now.strftime("%Yë…„ %mì›” %dì¼"),
            "current_time": now.strftime("%Hì‹œ %Më¶„"),
            "current_datetime": now.strftime("%Y-%m-%d %H:%M:%S"),
            "day_of_week": now.strftime("%A"),
            "korean_day": ["ì›”ìš”ì¼", "í™”ìš”ì¼", "ìˆ˜ìš”ì¼", "ëª©ìš”ì¼", "ê¸ˆìš”ì¼", "í† ìš”ì¼", "ì¼ìš”ì¼"][now.weekday()],
            "iso_format": now.isoformat()
        }
    
    def _analyze_deadline_status(self, application_period: str) -> Dict[str, Any]:
        """ì§€ì›ì‚¬ì—… ë§ˆê°ì¼ ë¶„ì„ (YYYYMMDD í˜•ì‹ í¬í•¨)"""
        try:
            kst = timezone(timedelta(hours=9))
            now = datetime.now(kst)
            
            # ì ‘ìˆ˜ê¸°ê°„ì—ì„œ ë§ˆê°ì¼ ì¶”ì¶œ ì‹œë„
            import re
            
            deadline_info = {
                "status": "unknown",
                "days_remaining": None,
                "deadline_date": None,
                "is_expired": False,
                "is_urgent": False
            }
            
            if not application_period:
                return deadline_info
            
            # YYYYMMDD ~ YYYYMMDD í˜•ì‹ ìš°ì„  ì²˜ë¦¬
            yyyymmdd_pattern = r'(\d{8})\s*~\s*(\d{8})'
            yyyymmdd_match = re.search(yyyymmdd_pattern, application_period)
            
            if yyyymmdd_match:
                try:
                    end_date_str = yyyymmdd_match.group(2)  # ë§ˆê°ì¼
                    year = int(end_date_str[:4])
                    month = int(end_date_str[4:6])
                    day = int(end_date_str[6:8])
                    
                    deadline = datetime(year, month, day, 23, 59, 59, tzinfo=kst)
                    days_diff = (deadline - now).days
                    
                    deadline_info.update({
                        "deadline_date": deadline.strftime("%Y-%m-%d"),
                        "days_remaining": days_diff,
                        "is_expired": days_diff < 0,
                        "is_urgent": 0 <= days_diff <= 3
                    })
                    
                    if days_diff < 0:
                        deadline_info["status"] = "expired"
                    elif days_diff == 0:
                        deadline_info["status"] = "today"
                    elif days_diff <= 3:
                        deadline_info["status"] = "urgent"
                    elif days_diff <= 7:
                        deadline_info["status"] = "soon"
                    else:
                        deadline_info["status"] = "active"
                        
                    return deadline_info
                    
                except ValueError:
                    pass
            
            # YYYY.MM.DD í˜•ì‹ ì²˜ë¦¬
            dot_pattern = r'(\d{4})\.(\d{1,2})\.(\d{1,2})\s*~\s*(\d{4})\.(\d{1,2})\.(\d{1,2})'
            dot_match = re.search(dot_pattern, application_period)
            
            if dot_match:
                try:
                    year = int(dot_match.group(4))
                    month = int(dot_match.group(5))
                    day = int(dot_match.group(6))
                    
                    deadline = datetime(year, month, day, 23, 59, 59, tzinfo=kst)
                    days_diff = (deadline - now).days
                    
                    deadline_info.update({
                        "deadline_date": deadline.strftime("%Y-%m-%d"),
                        "days_remaining": days_diff,
                        "is_expired": days_diff < 0,
                        "is_urgent": 0 <= days_diff <= 3
                    })
                    
                    if days_diff < 0:
                        deadline_info["status"] = "expired"
                    elif days_diff == 0:
                        deadline_info["status"] = "today"
                    elif days_diff <= 3:
                        deadline_info["status"] = "urgent"
                    elif days_diff <= 7:
                        deadline_info["status"] = "soon"
                    else:
                        deadline_info["status"] = "active"
                        
                    return deadline_info
                    
                except ValueError:
                    pass
            
            # ê¸°íƒ€ í˜•ì‹ë“¤ë„ ì‹œë„í•  ìˆ˜ ìˆì§€ë§Œ, ê¸°ë³¸ê°’ ë°˜í™˜
            return deadline_info
            
        except Exception as e:
            logger.error(f"ë§ˆê°ì¼ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                "status": "unknown",
                "days_remaining": None,
                "deadline_date": None,
                "is_expired": False,
                "is_urgent": False
            }
    
    def _initialize_openai(self):
        """OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        if not OPENAI_AVAILABLE:
            logger.warning("OpenAIë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        if not config.OPENAI_API_KEY:
            logger.warning("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
        
        try:
            self.openai_client = openai.OpenAI(api_key=config.OPENAI_API_KEY)
            logger.info("OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    @monitor_performance
    def get_response(self, user_query: str) -> Dict[str, Any]:
        """ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ RAG ê¸°ë°˜ ì‘ë‹µ ìƒì„± (ë©”ëª¨ë¦¬ ê¸°ëŠ¥ í¬í•¨)"""
        try:
            # 1. ì§ˆë¬¸ ì„ë² ë”© ìƒì„±
            query_embedding = self.embedding_manager.create_embedding(user_query)
            
            # 2. ì‹ ì²­ ê°€ëŠ¥í•œ ì§€ì›ì‚¬ì—… ìš°ì„  ê²€ìƒ‰
            search_results = self._search_with_application_priority(query_embedding, top_k=30, user_query=user_query)
            
            # 3. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (ê²€ìƒ‰ ê²°ê³¼ + ëŒ€í™” ê¸°ë¡)
            context = self._build_context(search_results)
            conversation_context = self._build_conversation_context()
            
            # 4. LLMì„ í†µí•œ ë‹µë³€ ìƒì„± (ë©”ëª¨ë¦¬ í¬í•¨)
            if self.openai_client:
                response_text = self._generate_response_with_memory(user_query, context, conversation_context)
            else:
                response_text = self._generate_fallback_response(user_query, search_results)
            
            # 5. ê²°ê³¼ êµ¬ì„± (ì‹ ì²­ ê°€ëŠ¥ ì—¬ë¶€ í†µê³„ í¬í•¨)
            applicable_count = len([r for r in search_results if r.get("is_applicable", False)])
            urgent_count = len([r for r in search_results if r.get("deadline_status", {}).get("is_urgent", False)])
            
            result = {
                "answer": response_text,
                "sources": self._extract_sources(search_results),
                "confidence": self._calculate_confidence(search_results),
                "context_used": bool(context),
                "memory_used": len(self.conversation_memory) > 0,
                "applicable_count": applicable_count,
                "urgent_count": urgent_count,
                "total_results": len(search_results)
            }
            
            # 6. ëŒ€í™” ê¸°ë¡ ë° ë©”ëª¨ë¦¬ ì—…ë°ì´íŠ¸
            self._add_to_chat_history("user", user_query)
            self._add_to_chat_history("assistant", response_text)
            self._update_conversation_memory(user_query, response_text)
            
            log_chatbot_interaction(
                user_query=user_query,
                response=response_text,
                confidence=result["confidence"],
                sources=result["sources"]
            )
            
            return result
            
        except Exception as e:
            logger.error(f"RAG ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            return {
                "answer": "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "sources": [],
                "confidence": 0.0,
                "context_used": False,
                "memory_used": False,
                "error": str(e)
            }
    
    def _build_context(self, search_results: List[Dict[str, Any]]) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ êµ¬ì„± (ëª¨ë“  ë©”íƒ€ë°ì´í„° í™œìš© + ë§ˆê°ì¼ ìƒíƒœ + ë°ì´í„° ì†ŒìŠ¤ êµ¬ë¶„)"""
        if not search_results:
            return ""
        
        contexts = []
        user_created_count = 0
        api_data_count = 0
        
        for i, result in enumerate(search_results, 1):
            metadata = result.get("metadata", {})
            
            # ë°ì´í„° ì†ŒìŠ¤ í™•ì¸ ë° ë¶„ë¥˜
            data_source = metadata.get('data_source', 'api_data')
            if data_source in ['user_created', 'user_updated']:
                user_created_count += 1
                source_emoji = "ğŸ‘¤"
                source_label = "ì‚¬ìš©ì ìƒì„±" if data_source == 'user_created' else "ì‚¬ìš©ì ìˆ˜ì •"
            else:
                api_data_count += 1
                source_emoji = "ğŸ›ï¸"
                source_label = "K-Startup ê³µì‹"
            
            # ë§ˆê°ì¼ ìƒíƒœ ë¶„ì„
            application_period = metadata.get('application_period', '')
            deadline_info = self._analyze_deadline_status(application_period)
            
            # ë§ˆê°ì¼ ìƒíƒœì— ë”°ë¥¸ ì´ëª¨ì§€ì™€ ë©”ì‹œì§€
            status_emoji = {
                "expired": "âŒ",
                "today": "ğŸš¨",
                "urgent": "âš ï¸",
                "soon": "â°",
                "active": "âœ…",
                "unknown": "â“"
            }
            
            status_message = {
                "expired": "ë§ˆê°ë¨",
                "today": "ì˜¤ëŠ˜ ë§ˆê°!",
                "urgent": f"ê¸´ê¸‰! {deadline_info['days_remaining']}ì¼ ë‚¨ìŒ",
                "soon": f"{deadline_info['days_remaining']}ì¼ ë‚¨ìŒ",
                "active": f"{deadline_info['days_remaining']}ì¼ ë‚¨ìŒ" if deadline_info['days_remaining'] else "ì‹ ì²­ ê°€ëŠ¥",
                "unknown": "ë§ˆê°ì¼ í™•ì¸ í•„ìš”"
            }
            
            deadline_status = f"{status_emoji.get(deadline_info['status'], 'â“')} {status_message.get(deadline_info['status'], 'ìƒíƒœ ë¶ˆëª…')}"
            
            context_piece = f"""
=== ì§€ì›ì‚¬ì—… {i} ===
{source_emoji} ë°ì´í„° ì¶œì²˜: {source_label}
ğŸ“¢ ì œëª©: {metadata.get('title', 'ì œëª© ì—†ìŒ')}
ğŸ¢ ê¸°ê´€: {metadata.get('organization', 'ê¸°ê´€ ì •ë³´ ì—†ìŒ')} ({metadata.get('department', 'ë¶€ì„œ ì •ë³´ ì—†ìŒ')})
ğŸ¯ ë¶„ì•¼: {metadata.get('support_field', 'ë¶„ì•¼ ì •ë³´ ì—†ìŒ')}
ğŸ‘¥ ëŒ€ìƒ: {metadata.get('target_audience', 'ëŒ€ìƒ ì •ë³´ ì—†ìŒ')}
ğŸ‘¶ ì—°ë ¹ëŒ€: {metadata.get('target_age', 'ì—°ë ¹ ì •ë³´ ì—†ìŒ')}
ğŸš€ ì°½ì—…ê²½í—˜: {metadata.get('startup_experience', 'ê²½í—˜ ì •ë³´ ì—†ìŒ')}
ğŸ“ ì§€ì—­: {metadata.get('region', 'ì§€ì—­ ì •ë³´ ì—†ìŒ')}
ğŸ“… ì ‘ìˆ˜ê¸°ê°„: {application_period}
â° ë§ˆê°ìƒíƒœ: {deadline_status}
ğŸ“ ì„¤ëª…: {metadata.get('description', 'ì„¤ëª… ì—†ìŒ')[:300]}...
ğŸ’° ì§€ì›ë‚´ìš©: {metadata.get('support_content', 'ì§€ì›ë‚´ìš© ì •ë³´ ì—†ìŒ')[:300]}...
ğŸ“‹ ì‹ ì²­ë°©ë²•: {metadata.get('application_method', 'ì‹ ì²­ë°©ë²• ì •ë³´ ì—†ìŒ')}
ğŸ“„ ì œì¶œì„œë¥˜: {metadata.get('submission_documents', 'ì œì¶œì„œë¥˜ ì •ë³´ ì—†ìŒ')}
ğŸ“ ì—°ë½ì²˜: {metadata.get('contact', 'ì—°ë½ì²˜ ì •ë³´ ì—†ìŒ')}
ğŸ“Š ìœ ì‚¬ë„: {result.get('score', 0.0):.3f}
            """.strip()
            
            contexts.append(context_piece)
        
        # í†µê³„ ì •ë³´ ì¶”ê°€
        stats_info = f"\nğŸ“ˆ ê²€ìƒ‰ í†µê³„: ì´ {len(search_results)}ê°œ ê²°ê³¼ (ğŸ‘¤ ì‚¬ìš©ì ìƒì„±: {user_created_count}ê°œ, ğŸ›ï¸ ê³µì‹ ë°ì´í„°: {api_data_count}ê°œ)"
        
        full_context = stats_info + "\n\n" + "\n\n".join(contexts)
        logger.info(f"[RAG í†µí•© ì»¨í…ìŠ¤íŠ¸ ë¡œê·¸] ê²€ìƒ‰ ê²°ê³¼ ì»¨í…ìŠ¤íŠ¸(ìƒìœ„ {len(contexts)}ê°œ, ì‚¬ìš©ì: {user_created_count}, ê³µì‹: {api_data_count}):\n{full_context}")
        return full_context
    
    def _build_conversation_context(self) -> str:
        """ëŒ€í™” ê¸°ë¡ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ êµ¬ì„±"""
        if not self.conversation_memory:
            return ""
        
        context_parts = ["ì´ì „ ëŒ€í™” ë‚´ìš©:"]
        for i, memory in enumerate(self.conversation_memory, 1):
            context_parts.append(f"ëŒ€í™” {i}:")
            context_parts.append(f"ì‚¬ìš©ì: {memory['user_query']}")
            context_parts.append(f"ë‹µë³€: {memory['response'][:100]}...")
            context_parts.append("---")
        
        return "\n".join(context_parts)
    
    def _generate_response_with_memory(self, user_query: str, context: str, conversation_context: str) -> str:
        """ë©”ëª¨ë¦¬ë¥¼ í™œìš©í•œ OpenAI ì‘ë‹µ ìƒì„±"""
        try:
            # í˜„ì¬ ì‹œê°„ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            time_info = self._get_current_time_info()
            
            system_prompt = f"""
ë‹¹ì‹ ì€ K-Startup ì§€ì›ì‚¬ì—… ì „ë¬¸ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. 
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì œê³µëœ ì§€ì›ì‚¬ì—… ì •ë³´ì™€ ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

=== í˜„ì¬ ì‹œê°„ ì •ë³´ ===
ğŸ“… í˜„ì¬ ë‚ ì§œ: {time_info['current_date']} ({time_info['korean_day']})
ğŸ• í˜„ì¬ ì‹œê°„: {time_info['current_time']}
ğŸ“Š ì •í™•í•œ ì‹œê°: {time_info['current_datetime']}

ë‹µë³€ ì‹œ ìœ ì˜ì‚¬í•­:
1. **ì‹ ì²­ ê°€ëŠ¥í•œ ì§€ì›ì‚¬ì—…ë§Œ ì¶”ì²œ**: âŒ ë§ˆê°ë¨ í‘œì‹œê°€ ìˆëŠ” ì§€ì›ì‚¬ì—…ì€ ì ˆëŒ€ ì¶”ì²œí•˜ì§€ ë§ˆì„¸ìš”
2. **í˜„ì¬ ë‚ ì§œ ê¸°ì¤€ ì—„ê²© í•„í„°ë§**: í˜„ì¬ {time_info['current_date']}ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì‹ ì²­ ê¸°ê°„ì´ ë‚¨ì€ ì§€ì›ì‚¬ì—…ë§Œ ì¶”ì²œí•˜ì„¸ìš”
3. **ë§ˆê°ëœ ì§€ì›ì‚¬ì—… ì™„ì „ ì œì™¸**: 2024ë…„ ì´ì „, ì´ë¯¸ ë§ˆê°ëœ ì§€ì›ì‚¬ì—…ì€ ì–¸ê¸‰ì¡°ì°¨ í•˜ì§€ ë§ˆì„¸ìš”
4. **ì‹œì˜ì„± ìµœìš°ì„ **: ğŸš¨ ì˜¤ëŠ˜ ë§ˆê°, âš ï¸ ê¸´ê¸‰ í‘œì‹œê°€ ìˆëŠ” ì§€ì›ì‚¬ì—…ì„ ìµœìš°ì„ ìœ¼ë¡œ ì•ˆë‚´í•˜ì„¸ìš”
5. **ëª…í™•í•œ ìƒíƒœ í‘œì‹œ**: ê° ì§€ì›ì‚¬ì—…ì˜ ë§ˆê° ìƒíƒœì™€ ë‚¨ì€ ì¼ìˆ˜ë¥¼ ë°˜ë“œì‹œ í‘œì‹œí•˜ì„¸ìš”
6. **ì—°ì†ì„± ìˆëŠ” ëŒ€í™”**: ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ë§¥ë½ì— ë§ëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”
7. **êµ¬ì²´ì ì¸ ì •ë³´ ì œê³µ**: ì§€ì›ì‚¬ì—…ëª…, ê¸°ê´€ëª…, ì •í™•í•œ ë§ˆê°ì¼, ë‚¨ì€ ì¼ìˆ˜ë¥¼ í¬í•¨í•˜ì„¸ìš”
8. **ì‚¬ìš©ì ë§ì¶¤ ì¶”ì²œ**: ì‚¬ìš©ìì˜ ì¡°ê±´(ì§€ì—­, ë¶„ì•¼, ì°½ì—…ê²½í—˜ ë“±)ì— ë§ëŠ” ì§€ì›ì‚¬ì—…ì„ ìš°ì„  ì¶”ì²œí•˜ì„¸ìš”
9. **ì‹¤ìš©ì  ì •ë³´ ì œê³µ**: ì‹ ì²­ ë°©ë²•, ì œì¶œ ì„œë¥˜, ì—°ë½ì²˜ ë“± ì¦‰ì‹œ í™œìš© ê°€ëŠ¥í•œ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”
10. **ì •í™•ì„± ìµœìš°ì„ **: ë¶ˆí™•ì‹¤í•œ ì •ë³´ë³´ë‹¤ëŠ” í™•ì‹¤í•˜ê³  ì‹ ì²­ ê°€ëŠ¥í•œ ì •ë³´ë§Œ ì œê³µí•˜ì„¸ìš”
11. **ê¸´ê¸‰ì„± ê°•ì¡°**: ë§ˆê°ì´ ì„ë°•í•œ ì§€ì›ì‚¬ì—…ì€ ë°˜ë“œì‹œ ê¸´ê¸‰ì„±ì„ ê°•ì¡°í•˜ì—¬ ì•ˆë‚´í•˜ì„¸ìš”
12. **ì¹œê·¼í•˜ê³  ì „ë¬¸ì ì¸ í†¤**: ìƒë‹´ì‚¬ë¡œì„œ ì‹¤ì§ˆì ìœ¼ë¡œ ë„ì›€ì´ ë˜ëŠ” ì¡°ì–¸ì„ ì œê³µí•˜ì„¸ìš”
13. **ê¸ˆì•¡ ì¡°ê±´ ìš°ì„  ì²˜ë¦¬**: ì‚¬ìš©ìê°€ íŠ¹ì • ê¸ˆì•¡ ì´ìƒì˜ ì§€ì›ì‚¬ì—…ì„ ìš”ì²­í•œ ê²½ìš°, í•´ë‹¹ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ì§€ì›ì‚¬ì—…ì„ ìµœìš°ì„ ìœ¼ë¡œ ì¶”ì²œí•˜ì„¸ìš”
14. **ê¸ˆì•¡ ì •ë³´ ëª…í™• í‘œì‹œ**: ê° ì§€ì›ì‚¬ì—…ì˜ ì§€ì› ê¸ˆì•¡ì„ ëª…í™•íˆ í‘œì‹œí•˜ê³ , ì‚¬ìš©ìê°€ ìš”ì²­í•œ ê¸ˆì•¡ ì¡°ê±´ê³¼ ë¹„êµí•˜ì—¬ ì„¤ëª…í•˜ì„¸ìš”

ë§ˆê°ì¼ ìƒíƒœ í‘œì‹œ ê°€ì´ë“œ:
- âŒ ë§ˆê°ë¨: ì´ë¯¸ ì ‘ìˆ˜ê°€ ì¢…ë£Œëœ ì§€ì›ì‚¬ì—…
- ğŸš¨ ì˜¤ëŠ˜ ë§ˆê°: ì˜¤ëŠ˜ì´ ë§ˆê°ì¼ì¸ ì§€ì›ì‚¬ì—… (ê¸´ê¸‰!)
- âš ï¸ ê¸´ê¸‰: 3ì¼ ì´ë‚´ ë§ˆê° ì˜ˆì •
- â° ê³§ ë§ˆê°: 7ì¼ ì´ë‚´ ë§ˆê° ì˜ˆì •
- âœ… ì‹ ì²­ ê°€ëŠ¥: ì—¬ìœ  ìˆê²Œ ì‹ ì²­ ê°€ëŠ¥í•œ ì§€ì›ì‚¬ì—…

ê¸ˆì•¡ ì¡°ê±´ ì²˜ë¦¬ ê°€ì´ë“œ:
- ğŸ’° ì‚¬ìš©ìê°€ ìš”ì²­í•œ ìµœì†Œ ê¸ˆì•¡ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ì§€ì›ì‚¬ì—…ì„ ìš°ì„  ì¶”ì²œ
- ğŸ’ ê¸ˆì•¡ì´ í° ì§€ì›ì‚¬ì—…ì¼ìˆ˜ë¡ ë” ìƒì„¸í•œ ì •ë³´ ì œê³µ
- ğŸ“Š ì§€ì› ê¸ˆì•¡ì„ ì •ê·œí™”ëœ í˜•íƒœ(ì˜ˆ: 1000ì–µì›, 500ì–µì›)ë¡œ í‘œì‹œ
- ğŸ¯ ê¸ˆì•¡ ì¡°ê±´ì„ ë§Œì¡±í•˜ì§€ ì•ŠëŠ” ì§€ì›ì‚¬ì—…ì€ í›„ìˆœìœ„ë¡œ ë°°ì¹˜í•˜ë˜, ê´€ë ¨ì„±ì´ ë†’ìœ¼ë©´ ì°¸ê³ ìš©ìœ¼ë¡œ ì–¸ê¸‰
            """
            
            # ë©”ì‹œì§€ êµ¬ì„±
            messages = [{"role": "system", "content": system_prompt}]
            
            # ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
            if conversation_context:
                messages.append({
                    "role": "system", 
                    "content": f"ì°¸ê³ í•  ì´ì „ ëŒ€í™”:\n{conversation_context}"
                })
            
            # í˜„ì¬ ì§ˆë¬¸ê³¼ ê²€ìƒ‰ ê²°ê³¼
            current_message = f"í˜„ì¬ ì§ˆë¬¸: {user_query}"
            if context:
                current_message += f"\n\nê´€ë ¨ ì§€ì›ì‚¬ì—… ì •ë³´:\n{context}"
            
            messages.append({"role": "user", "content": current_message})
            
            response = self.openai_client.chat.completions.create(
                model="gpt-4.1-mini",
                messages=messages,
                max_tokens=1800,
                temperature=0.7
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            logger.error(f"OpenAI ë©”ëª¨ë¦¬ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            return self._generate_fallback_response(user_query, [])
    
    def _search_with_application_priority(self, query_vector: List[float], top_k: int = 30, user_query: str = "") -> List[Dict[str, Any]]:
        """ì‹ ì²­ ê°€ëŠ¥í•œ ì§€ì›ì‚¬ì—…ì„ ìš°ì„ ì ìœ¼ë¡œ ê²€ìƒ‰ (ê¸ˆì•¡ ì¡°ê±´ í¬í•¨)"""
        try:
            # ì‚¬ìš©ì ì¿¼ë¦¬ì—ì„œ ê¸ˆì•¡ ì¡°ê±´ ì¶”ì¶œ
            amount_condition = _extract_amount_condition_from_query(user_query)
            
            # í•„í„°ë§ ì—†ì´ ì „ì²´ ê²€ìƒ‰ì„ í•œ ë‹¤ìŒ, ê²°ê³¼ë¥¼ í›„ì²˜ë¦¬ë¡œ ì •ë ¬
            # Pinecone í•„í„°ê°€ ì œëŒ€ë¡œ ì‘ë™í•˜ì§€ ì•ŠëŠ” ê²½ìš°ì— ëŒ€ë¹„
            
            # ë” ë§ì€ ê²°ê³¼ë¥¼ ê°€ì ¸ì™€ì„œ í›„ì²˜ë¦¬ë¡œ í•„í„°ë§
            all_results = self.pinecone_manager.search_similar(
                query_vector=query_vector,
                top_k=top_k * 4  # 4ë°° ë” ê°€ì ¸ì™€ì„œ í•„í„°ë§
            )
            
            # ì‹ ì²­ ê°€ëŠ¥í•œ ì§€ì›ì‚¬ì—…ê³¼ ë§Œë£Œëœ ì§€ì›ì‚¬ì—… ë¶„ë¥˜
            applicable_results = []
            expired_results = []
            current_year_results = []
            amount_matched_results = []
            
            current_year = datetime.now().year
            min_amount = amount_condition["min_amount"]
            
            for result in all_results:
                metadata = result.get("metadata", {})
                
                # í˜„ì¬ ì—°ë„ ì§€ì›ì‚¬ì—…ì¸ì§€ í™•ì¸
                is_current_year = result.get("is_current_year", False)
                is_applicable = result.get("is_applicable", False)
                
                # ê¸ˆì•¡ ì¡°ê±´ í™•ì¸
                amount_value = metadata.get("amount_value", 0)
                # amount_valueê°€ ë¬¸ìì—´ì¸ ê²½ìš° ì •ìˆ˜ë¡œ ë³€í™˜
                if isinstance(amount_value, str):
                    try:
                        amount_value = int(amount_value)
                    except (ValueError, TypeError):
                        amount_value = 0
                
                meets_amount_condition = (min_amount == 0) or (amount_value >= min_amount)
                
                if is_current_year:
                    current_year_results.append(result)
                
                if is_applicable:
                    applicable_results.append(result)
                else:
                    expired_results.append(result)
                
                if meets_amount_condition and amount_value > 0:
                    amount_matched_results.append(result)
                    # ê¸ˆì•¡ ì¡°ê±´ ë§Œì¡± í‘œì‹œ
                    result["meets_amount_condition"] = True
                else:
                    result["meets_amount_condition"] = False
            
            # ìš°ì„ ìˆœìœ„ ì •ë ¬: ê¸ˆì•¡ ì¡°ê±´ + ì‹ ì²­ ê°€ëŠ¥ + í˜„ì¬ ì—°ë„ > ì‹ ì²­ ê°€ëŠ¥ > í˜„ì¬ ì—°ë„ > ê¸°íƒ€
            def priority_sort_key(result):
                metadata = result.get("metadata", {})
                is_applicable = result.get("is_applicable", False)
                is_current_year = result.get("is_current_year", False)
                meets_amount_condition = result.get("meets_amount_condition", False)
                deadline_status = result.get("deadline_status", {})
                score = result.get("score", 0.0)
                amount_value = metadata.get("amount_value", 0)
                
                # ìš°ì„ ìˆœìœ„ ì ìˆ˜ ê³„ì‚°
                priority_score = 0
                
                # ê¸ˆì•¡ ì¡°ê±´ ë§Œì¡± ì‹œ ëŒ€í­ ê°€ì‚°ì 
                if meets_amount_condition:
                    priority_score += 2000
                    
                    # ê¸ˆì•¡ì´ í´ìˆ˜ë¡ ì¶”ê°€ ì ìˆ˜ (ë¡œê·¸ ìŠ¤ì¼€ì¼)
                    if amount_value > 0:
                        import math
                        amount_bonus = min(math.log10(amount_value / 100000000) * 100, 500)  # ìµœëŒ€ 500ì 
                        priority_score += amount_bonus
                
                # ê¸°ì¡´ ìš°ì„ ìˆœìœ„
                if is_applicable and is_current_year:
                    priority_score += 1000  # ìµœìš°ì„ 
                elif is_applicable:
                    priority_score += 500   # ì‹ ì²­ ê°€ëŠ¥
                elif is_current_year:
                    priority_score += 100   # í˜„ì¬ ì—°ë„
                else:
                    priority_score += 10    # ê¸°íƒ€
                
                # ê¸´ê¸‰ë„ ì¶”ê°€ ì ìˆ˜
                if deadline_status.get("status") == "today":
                    priority_score += 200
                elif deadline_status.get("status") == "urgent":
                    priority_score += 100
                elif deadline_status.get("status") == "soon":
                    priority_score += 50
                
                return (-priority_score, -score)  # ë†’ì€ ìš°ì„ ìˆœìœ„, ë†’ì€ ì ìˆ˜ ìˆœ
            
            # ì „ì²´ ê²°ê³¼ë¥¼ ìš°ì„ ìˆœìœ„ë¡œ ì •ë ¬
            all_results.sort(key=priority_sort_key)
            
            # ìƒìœ„ ê²°ê³¼ ì„ íƒ
            final_results = all_results[:top_k]
            
            # í†µê³„ ì •ë³´ ê³„ì‚°
            applicable_count = sum(1 for r in final_results if r.get("is_applicable", False))
            current_year_count = sum(1 for r in final_results if r.get("is_current_year", False))
            urgent_count = sum(1 for r in final_results 
                             if r.get("deadline_status", {}).get("status") in ["today", "urgent"])
            amount_matched_count = sum(1 for r in final_results if r.get("meets_amount_condition", False))
            
            # ë¡œê·¸ ë©”ì‹œì§€ì— ê¸ˆì•¡ ì¡°ê±´ ì •ë³´ ì¶”ê°€
            log_message = f"ìµœì¢… ê²€ìƒ‰ ê²°ê³¼: ì´ {len(final_results)}ê°œ "
            log_message += f"(ì‹ ì²­ê°€ëŠ¥: {applicable_count}ê°œ, í˜„ì¬ì—°ë„: {current_year_count}ê°œ, ê¸´ê¸‰: {urgent_count}ê°œ"
            
            if amount_condition["min_amount"] > 0:
                log_message += f", ê¸ˆì•¡ì¡°ê±´ë§Œì¡±: {amount_matched_count}ê°œ"
                log_message += f", ìµœì†Œê¸ˆì•¡: {amount_condition['normalized_text'] if 'normalized_text' in amount_condition else _normalize_amount(str(amount_condition['min_amount']))['normalized_text']}"
            
            log_message += ")"
            logger.info(log_message)
            
            return final_results
            
        except Exception as e:
            logger.error(f"ìš°ì„ ìˆœìœ„ ê²€ìƒ‰ ì‹¤íŒ¨, ê¸°ë³¸ ê²€ìƒ‰ìœ¼ë¡œ ëŒ€ì²´: {e}")
            # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ê²€ìƒ‰ìœ¼ë¡œ ëŒ€ì²´
            return self.pinecone_manager.search_similar(
                query_vector=query_vector,
                top_k=top_k
            )
    
    def _update_conversation_memory(self, user_query: str, response: str):
        """ëŒ€í™” ë©”ëª¨ë¦¬ ì—…ë°ì´íŠ¸"""
        # ìƒˆë¡œìš´ ëŒ€í™” ì¶”ê°€
        memory_entry = {
            "user_query": user_query,
            "response": response,
            "timestamp": datetime.now().isoformat(),
            "turn": len(self.conversation_memory) + 1
        }
        
        self.conversation_memory.append(memory_entry)
        
        # ìµœëŒ€ í„´ ìˆ˜ ì œí•œ (ìµœê·¼ 5ê°œ ëŒ€í™”ë§Œ ìœ ì§€)
        if len(self.conversation_memory) > self.max_memory_turns:
            self.conversation_memory = self.conversation_memory[-self.max_memory_turns:]
            # í„´ ë²ˆí˜¸ ì¬ì¡°ì •
            for i, memory in enumerate(self.conversation_memory):
                memory["turn"] = i + 1
        
        logger.info(f"ëŒ€í™” ë©”ëª¨ë¦¬ ì—…ë°ì´íŠ¸: {len(self.conversation_memory)}ê°œ ëŒ€í™” ê¸°ì–µ ì¤‘")
    
    def get_conversation_summary(self) -> str:
        """ëŒ€í™” ìš”ì•½ ë°˜í™˜"""
        if not self.conversation_memory:
            return "ì•„ì§ ëŒ€í™” ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤."
        
        summary_parts = [f"ì´ {len(self.conversation_memory)}ê°œì˜ ëŒ€í™”ë¥¼ ê¸°ì–µí•˜ê³  ìˆìŠµë‹ˆë‹¤:\n"]
        
        for memory in self.conversation_memory:
            summary_parts.append(f"ëŒ€í™” {memory['turn']}: {memory['user_query'][:50]}...")
        
        return "\n".join(summary_parts)
    
    def clear_conversation_memory(self):
        """ëŒ€í™” ë©”ëª¨ë¦¬ ì´ˆê¸°í™”"""
        self.conversation_memory.clear()
        logger.info("ëŒ€í™” ë©”ëª¨ë¦¬ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def get_memory_status(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ìƒíƒœ ë°˜í™˜"""
        return {
            "total_conversations": len(self.conversation_memory),
            "max_memory_turns": self.max_memory_turns,
            "memory_usage": f"{len(self.conversation_memory)}/{self.max_memory_turns}",
            "oldest_conversation": self.conversation_memory[0]["timestamp"] if self.conversation_memory else None,
            "latest_conversation": self.conversation_memory[-1]["timestamp"] if self.conversation_memory else None
        }
    
    def _generate_fallback_response(self, user_query: str, search_results: List[Dict[str, Any]]) -> str:
        """ëŒ€ì²´ ì‘ë‹µ ìƒì„± (OpenAI ì—†ì´, í˜„ì¬ ì‹œê°„ ì •ë³´ í¬í•¨)"""
        # í˜„ì¬ ì‹œê°„ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        time_info = self._get_current_time_info()
        
        if not search_results:
            return f"""
í˜„ì¬ ì‹œê°„: {time_info['current_date']} {time_info['current_time']}

ì£„ì†¡í•©ë‹ˆë‹¤. '{user_query}'ì™€ ê´€ë ¨ëœ ì§€ì›ì‚¬ì—… ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 
ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ë‹¤ì‹œ ì‹œë„í•´ë³´ì‹œê±°ë‚˜, ë” êµ¬ì²´ì ì¸ ì¡°ê±´ì„ ë§ì”€í•´ ì£¼ì„¸ìš”.

ì˜ˆì‹œ: "AI ìŠ¤íƒ€íŠ¸ì—…", "ì„œìš¸ ì§€ì—­", "3ë…„ ë¯¸ë§Œ ê¸°ì—…" ë“±
            """.strip()
        
        # ê°€ì¥ ìœ ì‚¬ë„ê°€ ë†’ì€ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°„ë‹¨í•œ ì‘ë‹µ ìƒì„±
        best_match = search_results[0]
        metadata = best_match.get("metadata", {})
        
        # ë§ˆê°ì¼ ìƒíƒœ ë¶„ì„
        application_period = metadata.get('application_period', '')
        deadline_info = self._analyze_deadline_status(application_period)
        
        # ë§ˆê°ì¼ ìƒíƒœ ë©”ì‹œì§€
        status_messages = {
            "expired": "âŒ ì´ë¯¸ ë§ˆê°ëœ ì§€ì›ì‚¬ì—…ì…ë‹ˆë‹¤",
            "today": "ğŸš¨ ì˜¤ëŠ˜ ë§ˆê°! ê¸´ê¸‰íˆ ì‹ ì²­í•˜ì„¸ìš”",
            "urgent": f"âš ï¸ ê¸´ê¸‰! {deadline_info['days_remaining']}ì¼ ë‚¨ì•˜ìŠµë‹ˆë‹¤",
            "soon": f"â° {deadline_info['days_remaining']}ì¼ ë‚¨ì•˜ìŠµë‹ˆë‹¤",
            "active": f"âœ… ì‹ ì²­ ê°€ëŠ¥ ({deadline_info['days_remaining']}ì¼ ë‚¨ìŒ)" if deadline_info['days_remaining'] else "âœ… ì‹ ì²­ ê°€ëŠ¥",
            "unknown": "â“ ë§ˆê°ì¼ì„ í™•ì¸í•´ ì£¼ì„¸ìš”"
        }
        
        deadline_status = status_messages.get(deadline_info['status'], 'ìƒíƒœ ë¶ˆëª…')
        
        response = f"""
í˜„ì¬ ì‹œê°„: {time_info['current_date']} {time_info['current_time']}

'{user_query}'ì™€ ê´€ë ¨í•˜ì—¬ ë‹¤ìŒ ì§€ì›ì‚¬ì—…ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤:

ğŸ“Œ **{metadata.get('title', 'ì œëª© ì •ë³´ ì—†ìŒ')}**
ğŸ¢ ì£¼ê´€ê¸°ê´€: {metadata.get('organization', 'ê¸°ê´€ ì •ë³´ ì—†ìŒ')}
ğŸ“… ì ‘ìˆ˜ê¸°ê°„: {application_period}
â° ë§ˆê°ìƒíƒœ: {deadline_status}
ğŸ¯ ë¶„ì•¼: {metadata.get('support_field', 'ë¶„ì•¼ ì •ë³´ ì—†ìŒ')}
ğŸ‘¥ ëŒ€ìƒ: {metadata.get('target_audience', 'ëŒ€ìƒ ì •ë³´ ì—†ìŒ')}

ğŸ“ ì„¤ëª…: {metadata.get('description', 'ìƒì„¸ ì„¤ëª…ì´ ì—†ìŠµë‹ˆë‹¤.')[:200]}...

ë” ìì„¸í•œ ì •ë³´ëŠ” í•´ë‹¹ ê¸°ê´€ì— ì§ì ‘ ë¬¸ì˜í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.
ğŸ“ ì—°ë½ì²˜: {metadata.get('contact', 'ì—°ë½ì²˜ ì •ë³´ ì—†ìŒ')}
        """.strip()
        
        return response
    
    def _extract_sources(self, search_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì†ŒìŠ¤ ì •ë³´ ì¶”ì¶œ"""
        sources = []
        for result in search_results:
            metadata = result.get("metadata", {})
            sources.append({
                "title": metadata.get("title", "ì œëª© ì—†ìŒ"),
                "organization": metadata.get("organization", "ê¸°ê´€ ì •ë³´ ì—†ìŒ"),
                "score": result.get("score", 0.0),
                "id": result.get("id", "")
            })
        return sources
    
    def _calculate_confidence(self, search_results: List[Dict[str, Any]]) -> float:
        """ì‘ë‹µ ì‹ ë¢°ë„ ê³„ì‚° (Pinecone ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ì¤€)"""
        if not search_results:
            return 0.0
        
        # ê°€ì¥ ë†’ì€ ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì‹ ë¢°ë„ ê³„ì‚°
        max_score = max(result.get("score", 0.0) for result in search_results)
        
        # Pinecone ì½”ì‚¬ì¸ ìœ ì‚¬ë„ì— ë§ê²Œ ì„ê³„ê°’ ì¡°ì •
        # 0.6 ì´ìƒ: ë§¤ìš° ë†’ì€ ì‹ ë¢°ë„ (85-100%)
        # 0.4-0.6: ë†’ì€ ì‹ ë¢°ë„ (60-85%)
        # 0.2-0.4: ë³´í†µ ì‹ ë¢°ë„ (30-60%)
        # 0.2 ë¯¸ë§Œ: ë‚®ì€ ì‹ ë¢°ë„ (0-30%)
        
        if max_score >= 0.6:
            # 0.6 ì´ìƒì€ 85-100% ì‹ ë¢°ë„
            return min(0.85 + (max_score - 0.6) * 0.375, 1.0)  # 0.6->85%, 1.0->100%
        elif max_score >= 0.4:
            # 0.4-0.6ì€ 60-85% ì‹ ë¢°ë„
            return 0.6 + (max_score - 0.4) * 1.25  # 0.4->60%, 0.6->85%
        elif max_score >= 0.2:
            # 0.2-0.4ëŠ” 30-60% ì‹ ë¢°ë„
            return 0.3 + (max_score - 0.2) * 1.5  # 0.2->30%, 0.4->60%
        else:
            # 0.2 ë¯¸ë§Œì€ 0-30% ì‹ ë¢°ë„
            return max_score * 1.5  # 0->0%, 0.2->30%
    
    def _add_to_chat_history(self, role: str, content: str):
        """ëŒ€í™” ê¸°ë¡ ì¶”ê°€"""
        self.chat_history.append({
            "role": role,
            "content": content,
            "timestamp": datetime.now().isoformat()
        })
        
        # ê¸°ë¡ ê¸¸ì´ ì œí•œ
        if len(self.chat_history) > config.MAX_CHAT_HISTORY * 2:  # user + assistant
            self.chat_history = self.chat_history[-config.MAX_CHAT_HISTORY * 2:]
    
    def get_chat_history(self) -> List[Dict[str, Any]]:
        """ëŒ€í™” ê¸°ë¡ ë°˜í™˜"""
        return self.chat_history.copy()
    
    def clear_chat_history(self):
        """ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”"""
        self.chat_history.clear()
        logger.info("ëŒ€í™” ê¸°ë¡ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def get_system_status(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ìƒíƒœ ë°˜í™˜"""
        return {
            "embedding_model_loaded": self.embedding_manager.model is not None,
            "pinecone_connected": self.pinecone_manager.index is not None,
            "openai_available": self.openai_client is not None,
            "chat_history_length": len(self.chat_history),
            "pinecone_stats": self.pinecone_manager.get_index_stats()
        }

# ì „ì—­ RAG ì‹œìŠ¤í…œ ì¸ìŠ¤í„´ìŠ¤
_rag_chatbot = None

def get_rag_chatbot() -> RAGChatbot:
    """RAG ì±—ë´‡ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _rag_chatbot
    if _rag_chatbot is None:
        _rag_chatbot = RAGChatbot()
    return _rag_chatbot

def _normalize_amount(text: str) -> Dict[str, Any]:
    """
    ê¸ˆì•¡ ì •ë³´ë¥¼ ì •ê·œí™”í•˜ì—¬ ìˆ«ìì™€ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ (ê°•í™”ëœ íŒ¨í„´ ë§¤ì¹­)
    
    Args:
        text: ê¸ˆì•¡ì´ í¬í•¨ëœ í…ìŠ¤íŠ¸
        
    Returns:
        Dict: {
            "amount_value": int,  # ì› ë‹¨ìœ„ ê¸ˆì•¡
            "amount_text": str,   # ì›ë³¸ í…ìŠ¤íŠ¸
            "normalized_text": str,  # ì •ê·œí™”ëœ í…ìŠ¤íŠ¸
            "amount_type": str,   # ê¸ˆì•¡ ìœ í˜• (ì •í™•, ìµœëŒ€, ìµœì†Œ, ì•½)
            "all_amounts": List[Dict]  # ë°œê²¬ëœ ëª¨ë“  ê¸ˆì•¡ë“¤
        }
    """
    if not text:
        return {
            "amount_value": 0, 
            "amount_text": "", 
            "normalized_text": "",
            "amount_type": "none",
            "all_amounts": []
        }
    
    # ê°•í™”ëœ ê¸ˆì•¡ íŒ¨í„´ ì •ì˜ (ìš°ì„ ìˆœìœ„ ìˆœì„œ)
    patterns = [
        # === ì¡°ì› ë‹¨ìœ„ ===
        # 1000ì¡°ì›, 1,000ì¡°ì›, ì²œì¡°ì›, 1ì²œì¡°ì›
        (r'(\d{1,4}(?:,\d{3})*)\s*ì¡°\s*ì›?', lambda m: int(m.group(1).replace(',', '')) * 1000000000000, "ì¡°ì›"),
        (r'(\d+)\s*ì²œ\s*ì¡°\s*ì›?', lambda m: int(m.group(1)) * 1000000000000000, "ì²œì¡°ì›"),
        (r'ì²œ\s*ì¡°\s*ì›?', lambda m: 1000000000000000, "ì²œì¡°ì›"),
        
        # === ì–µì› ë‹¨ìœ„ ===
        # 1000ì–µì›, 1,000ì–µì›, ì²œì–µì›, 1ì²œì–µì›
        (r'(\d{1,4}(?:,\d{3})*)\s*ì–µ\s*ì›?', lambda m: int(m.group(1).replace(',', '')) * 100000000, "ì–µì›"),
        (r'(\d+)\s*ì²œ\s*ì–µ\s*ì›?', lambda m: int(m.group(1)) * 100000000000, "ì²œì–µì›"),
        (r'ì²œ\s*ì–µ\s*ì›?', lambda m: 100000000000, "ì²œì–µì›"),
        
        # === ë§Œì› ë‹¨ìœ„ ===
        # 1000ë§Œì›, 1,000ë§Œì›, ì²œë§Œì›, 1ì²œë§Œì›
        (r'(\d{1,4}(?:,\d{3})*)\s*ë§Œ\s*ì›?', lambda m: int(m.group(1).replace(',', '')) * 10000, "ë§Œì›"),
        (r'(\d+)\s*ì²œ\s*ë§Œ\s*ì›?', lambda m: int(m.group(1)) * 10000000, "ì²œë§Œì›"),
        (r'ì²œ\s*ë§Œ\s*ì›?', lambda m: 10000000, "ì²œë§Œì›"),
        (r'(\d+)\s*ë°±\s*ë§Œ\s*ì›?', lambda m: int(m.group(1)) * 1000000, "ë°±ë§Œì›"),
        (r'ë°±\s*ë§Œ\s*ì›?', lambda m: 1000000, "ë°±ë§Œì›"),
        
        # === ì› ë‹¨ìœ„ ===
        # 1,000,000ì› í˜•íƒœ
        (r'(\d{1,3}(?:,\d{3})+)\s*ì›', lambda m: int(m.group(1).replace(',', '')), "ì›"),
        # 1000000ì› í˜•íƒœ (7ìë¦¬ ì´ìƒ)
        (r'(\d{7,})\s*ì›', lambda m: int(m.group(1)), "ì›"),
        
        # === íŠ¹ìˆ˜ í‘œí˜„ ===
        # ìˆ˜ì‹­ì–µ, ìˆ˜ë°±ì–µ, ìˆ˜ì²œì–µ
        (r'ìˆ˜\s*ì‹­\s*ì–µ\s*ì›?', lambda m: 50000000000, "ìˆ˜ì‹­ì–µì›"),  # í‰ê·  50ì–µìœ¼ë¡œ ì¶”ì •
        (r'ìˆ˜\s*ë°±\s*ì–µ\s*ì›?', lambda m: 500000000000, "ìˆ˜ë°±ì–µì›"),  # í‰ê·  500ì–µìœ¼ë¡œ ì¶”ì •
        (r'ìˆ˜\s*ì²œ\s*ì–µ\s*ì›?', lambda m: 5000000000000, "ìˆ˜ì²œì–µì›"),  # í‰ê·  5ì²œì–µìœ¼ë¡œ ì¶”ì •
        (r'ìˆ˜\s*ì¡°\s*ì›?', lambda m: 5000000000000, "ìˆ˜ì¡°ì›"),  # í‰ê·  5ì¡°ë¡œ ì¶”ì •
        
        # ì‹­ì–µ, ë°±ì–µ, ì²œì–µ
        (r'ì‹­\s*ì–µ\s*ì›?', lambda m: 1000000000, "ì‹­ì–µì›"),
        (r'ë°±\s*ì–µ\s*ì›?', lambda m: 10000000000, "ë°±ì–µì›"),
        
        # === ë²”ìœ„ í‘œí˜„ ===
        # 10ì–µ~100ì–µì›
        (r'(\d+)\s*ì–µ?\s*~\s*(\d+)\s*ì–µ\s*ì›?', lambda m: int(m.group(2)) * 100000000, "ë²”ìœ„_ì–µì›"),
        (r'(\d+)\s*ë§Œ?\s*~\s*(\d+)\s*ë§Œ\s*ì›?', lambda m: int(m.group(2)) * 10000, "ë²”ìœ„_ë§Œì›"),
        
        # === í•œê¸€ ìˆ«ì ===
        # ì¼, ì´, ì‚¼, ì‚¬, ì˜¤, ìœ¡, ì¹ , íŒ”, êµ¬, ì‹­
        (r'([ì¼ì´ì‚¼ì‚¬ì˜¤ìœ¡ì¹ íŒ”êµ¬]?ì‹­?)\s*ì–µ\s*ì›?', lambda m: _korean_to_number(m.group(1)) * 100000000, "í•œê¸€_ì–µì›"),
        (r'([ì¼ì´ì‚¼ì‚¬ì˜¤ìœ¡ì¹ íŒ”êµ¬]?ì‹­?)\s*ë§Œ\s*ì›?', lambda m: _korean_to_number(m.group(1)) * 10000, "í•œê¸€_ë§Œì›"),
    ]
    
    # ìˆ˜ì‹ì–´ íŒ¨í„´ (ìµœëŒ€, ìµœì†Œ, ì•½ ë“±)
    modifier_patterns = [
        (r'ìµœëŒ€\s*(\d+(?:,\d{3})*)\s*(ì¡°|ì–µ|ë§Œ)?\s*ì›?', "ìµœëŒ€"),
        (r'ìµœê³ \s*(\d+(?:,\d{3})*)\s*(ì¡°|ì–µ|ë§Œ)?\s*ì›?', "ìµœëŒ€"),
        (r'ìµœì†Œ\s*(\d+(?:,\d{3})*)\s*(ì¡°|ì–µ|ë§Œ)?\s*ì›?', "ìµœì†Œ"),
        (r'ìµœì €\s*(\d+(?:,\d{3})*)\s*(ì¡°|ì–µ|ë§Œ)?\s*ì›?', "ìµœì†Œ"),
        (r'ì•½\s*(\d+(?:,\d{3})*)\s*(ì¡°|ì–µ|ë§Œ)?\s*ì›?', "ì•½"),
        (r'ëŒ€ëµ\s*(\d+(?:,\d{3})*)\s*(ì¡°|ì–µ|ë§Œ)?\s*ì›?', "ì•½"),
        (r'ì´\s*(\d+(?:,\d{3})*)\s*(ì¡°|ì–µ|ë§Œ)?\s*ì›?', "ì´"),
        (r'ì „ì²´\s*(\d+(?:,\d{3})*)\s*(ì¡°|ì–µ|ë§Œ)?\s*ì›?', "ì´"),
        (r'ê·œëª¨\s*(\d+(?:,\d{3})*)\s*(ì¡°|ì–µ|ë§Œ)?\s*ì›?', "ê·œëª¨"),
    ]
    
    all_amounts = []
    max_amount = 0
    max_amount_text = ""
    amount_type = "ì •í™•"
    
    # ìˆ˜ì‹ì–´ê°€ ìˆëŠ” íŒ¨í„´ ë¨¼ì € í™•ì¸
    for modifier_pattern, modifier_type in modifier_patterns:
        matches = re.finditer(modifier_pattern, text, re.IGNORECASE)
        for match in matches:
            try:
                number_str = match.group(1).replace(',', '')
                unit = match.group(2) if len(match.groups()) > 1 and match.group(2) else ""
                
                number = int(number_str)
                
                # ë‹¨ìœ„ ì ìš©
                if unit == "ì¡°":
                    amount = number * 1000000000000
                elif unit == "ì–µ":
                    amount = number * 100000000
                elif unit == "ë§Œ":
                    amount = number * 10000
                else:
                    # ë‹¨ìœ„ê°€ ì—†ìœ¼ë©´ ìˆ«ì í¬ê¸°ë¡œ ì¶”ì •
                    if number >= 1000000:
                        amount = number  # ì´ë¯¸ ì› ë‹¨ìœ„
                    elif number >= 1000:
                        amount = number * 10000  # ë§Œì› ë‹¨ìœ„ë¡œ ì¶”ì •
                    else:
                        amount = number * 100000000  # ì–µì› ë‹¨ìœ„ë¡œ ì¶”ì •
                
                all_amounts.append({
                    "amount": amount,
                    "text": match.group(0),
                    "type": modifier_type,
                    "unit": unit or "ì¶”ì •"
                })
                
                if amount > max_amount:
                    max_amount = amount
                    max_amount_text = match.group(0)
                    amount_type = modifier_type
                    
            except (ValueError, AttributeError):
                continue
    
    # ì¼ë°˜ íŒ¨í„´ í™•ì¸
    for pattern, converter, unit_type in patterns:
        matches = re.finditer(pattern, text, re.IGNORECASE)
        for match in matches:
            try:
                amount = converter(match)
                
                all_amounts.append({
                    "amount": amount,
                    "text": match.group(0),
                    "type": "ì •í™•",
                    "unit": unit_type
                })
                
                if amount > max_amount:
                    max_amount = amount
                    max_amount_text = match.group(0)
                    if amount_type == "ì •í™•":  # ìˆ˜ì‹ì–´ê°€ ì—†ì—ˆë‹¤ë©´
                        amount_type = "ì •í™•"
                        
            except (ValueError, AttributeError):
                continue
    
    # ì •ê·œí™”ëœ í…ìŠ¤íŠ¸ ìƒì„±
    normalized_text = ""
    if max_amount > 0:
        if max_amount >= 1000000000000:  # ì¡° ë‹¨ìœ„
            if max_amount % 1000000000000 == 0:
                normalized_text = f"{max_amount // 1000000000000}ì¡°ì›"
            else:
                normalized_text = f"{max_amount // 1000000000000}.{(max_amount % 1000000000000) // 100000000000}ì¡°ì›"
        elif max_amount >= 100000000:  # ì–µ ë‹¨ìœ„
            if max_amount % 100000000 == 0:
                normalized_text = f"{max_amount // 100000000}ì–µì›"
            else:
                normalized_text = f"{max_amount // 100000000}.{(max_amount % 100000000) // 10000000}ì–µì›"
        elif max_amount >= 10000:  # ë§Œì› ë‹¨ìœ„
            if max_amount % 10000 == 0:
                normalized_text = f"{max_amount // 10000}ë§Œì›"
            else:
                normalized_text = f"{max_amount // 10000}.{(max_amount % 10000) // 1000}ë§Œì›"
        else:
            normalized_text = f"{max_amount:,}ì›"
        
        # ìˆ˜ì‹ì–´ ì¶”ê°€
        if amount_type != "ì •í™•":
            normalized_text = f"{amount_type} {normalized_text}"
    
    return {
        "amount_value": max_amount,
        "amount_text": max_amount_text,
        "normalized_text": normalized_text,
        "amount_type": amount_type,
        "all_amounts": all_amounts
    }

def _korean_to_number(korean_text: str) -> int:
    """í•œê¸€ ìˆ«ìë¥¼ ì•„ë¼ë¹„ì•„ ìˆ«ìë¡œ ë³€í™˜"""
    if not korean_text:
        return 1
    
    korean_numbers = {
        'ì¼': 1, 'ì´': 2, 'ì‚¼': 3, 'ì‚¬': 4, 'ì˜¤': 5,
        'ìœ¡': 6, 'ì¹ ': 7, 'íŒ”': 8, 'êµ¬': 9, 'ì‹­': 10
    }
    
    if korean_text in korean_numbers:
        return korean_numbers[korean_text]
    
    # ì‹­ì˜ ë°°ìˆ˜ ì²˜ë¦¬ (ì´ì‹­, ì‚¼ì‹­ ë“±)
    if 'ì‹­' in korean_text and len(korean_text) == 2:
        prefix = korean_text[0]
        if prefix in korean_numbers:
            return korean_numbers[prefix] * 10
    
    return 1  # ê¸°ë³¸ê°’

def _extract_key_amounts(text: str) -> List[str]:
    """
    í…ìŠ¤íŠ¸ì—ì„œ ì£¼ìš” ê¸ˆì•¡ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
    """
    if not text:
        return []
    
    amount_info = _normalize_amount(text)
    amounts = []
    
    if amount_info["normalized_text"]:
        amounts.append(amount_info["normalized_text"])
    
    if amount_info["amount_text"] and amount_info["amount_text"] != amount_info["normalized_text"]:
        amounts.append(amount_info["amount_text"])
    
    return amounts

def ingest_announcements_to_pinecone(announcements_data: Dict[str, Any]) -> Tuple[bool, str]:
    """
    ì§€ì›ì‚¬ì—… ë°ì´í„°ë¥¼ Pineconeì— ì„ë² ë”©í•˜ì—¬ ì €ì¥ (ëª¨ë“  ë°ì´í„° ì†ŒìŠ¤ í†µí•©)
    
    Args:
        announcements_data: ì§€ì›ì‚¬ì—… ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        
    Returns:
        Tuple[bool, str]: (ì„±ê³µ ì—¬ë¶€, ë©”ì‹œì§€)
    """
    try:
        logger.info("ğŸ”„ í†µí•© ë°ì´í„° Pinecone ì €ì¥ ì‹œì‘...")
        
        # RAG ì‹œìŠ¤í…œ ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
        chatbot = get_rag_chatbot()
        
        if not chatbot.embedding_manager.model:
            return False, "ì„ë² ë”© ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        if not chatbot.pinecone_manager.index:
            # Pinecone ì¬ì´ˆê¸°í™” ì‹œë„
            logger.info("Pinecone ì¸ë±ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•„ ì¬ì´ˆê¸°í™”ë¥¼ ì‹œë„í•©ë‹ˆë‹¤...")
            try:
                chatbot.pinecone_manager._initialize_pinecone()
                if not chatbot.pinecone_manager.index:
                    return False, "Pinecone ì¸ë±ìŠ¤ ì¬ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
                logger.info("Pinecone ì¸ë±ìŠ¤ ì¬ì´ˆê¸°í™” ì„±ê³µ")
            except Exception as e:
                return False, f"Pinecone ì¸ë±ìŠ¤ ì¬ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}"
        
        # ğŸ”¥ ëª¨ë“  ë°ì´í„° ì†ŒìŠ¤ í†µí•© ë¡œë“œ
        all_data_sources = {}
        
        # 1. ì „ë‹¬ë°›ì€ ë°ì´í„° (K-Startup API ë°ì´í„°)
        if announcements_data:
            logger.info(f"ğŸ“Š K-Startup API ë°ì´í„°: {len(announcements_data)}ê°œ")
            all_data_sources.update(announcements_data)
        
        # 2. ì‚¬ìš©ì ìƒì„± ë°ì´í„° ì¶”ê°€ ë¡œë“œ
        try:
            import data_handler
            # data_handlerì—ì„œ ëª¨ë“  ë°ì´í„° ë¡œë“œ (ì‚¬ìš©ì ìƒì„± + API ë°ì´í„° ëª¨ë‘ í¬í•¨)
            data_handler.load_all_data()
            user_contests = data_handler.get_all_contests()
            
            if user_contests:
                logger.info(f"ğŸ‘¤ ì‚¬ìš©ì ìƒì„± ë°ì´í„°: {len(user_contests)}ê°œ")
                
                # ì‚¬ìš©ì ë°ì´í„°ë¥¼ í†µí•© ë°ì´í„°ì— ì¶”ê°€ (ì¤‘ë³µ ì œê±°)
                for contest in user_contests:
                    if isinstance(contest, dict):
                        contest_id = contest.get('pblancId', contest.get('id', str(len(all_data_sources))))
                        
                        # ë°ì´í„° ì†ŒìŠ¤ í‘œì‹œ ì¶”ê°€
                        contest_copy = contest.copy()
                        if contest_id not in all_data_sources:
                            contest_copy['data_source'] = 'user_created'
                            all_data_sources[str(contest_id)] = contest_copy
                        else:
                            # ê¸°ì¡´ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì‚¬ìš©ì ë°ì´í„°ë¡œ ì—…ë°ì´íŠ¸ (ìµœì‹  ì •ë³´ ìš°ì„ )
                            existing_data = all_data_sources[str(contest_id)]
                            existing_data.update(contest_copy)
                            existing_data['data_source'] = 'user_updated'
                            logger.info(f"ğŸ”„ ê¸°ì¡´ ë°ì´í„° ì—…ë°ì´íŠ¸: {contest_id}")
            
        except Exception as e:
            logger.warning(f"ì‚¬ìš©ì ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {e}")
        
        # 3. announcements.jsonì—ì„œ ì¶”ê°€ ë°ì´í„° ë¡œë“œ
        try:
            import os
            announcements_file = "announcements.json"
            if os.path.exists(announcements_file):
                with open(announcements_file, 'r', encoding='utf-8') as f:
                    announcements_json = json.load(f)
                    if announcements_json:
                        logger.info(f"ğŸ“„ announcements.json ë°ì´í„°: {len(announcements_json)}ê°œ")
                        
                        for ann_id, ann_data in announcements_json.items():
                            if str(ann_id) not in all_data_sources:
                                ann_data_copy = ann_data.copy()
                                ann_data_copy['data_source'] = 'announcements_json'
                                all_data_sources[str(ann_id)] = ann_data_copy
        except Exception as e:
            logger.warning(f"announcements.json ë¡œë“œ ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {e}")
        
        logger.info(f"ğŸ¯ í†µí•© ë°ì´í„° ì´ê³„: {len(all_data_sources)}ê°œ")
        
        # ë²¡í„° ë°ì´í„° ì¤€ë¹„
        vectors_to_upsert = []
        processed_count = 0
        skipped_count = 0
        user_created_count = 0
        api_data_count = 0
        
        logger.info(f"ğŸ“ ì´ {len(all_data_sources)}ê°œì˜ í†µí•© ê³µê³  ë°ì´í„° ì²˜ë¦¬ ì‹œì‘...")
        
        for announcement_id, announcement in all_data_sources.items():
            try:
                # ë°ì´í„° ì†ŒìŠ¤ ë¶„ë¥˜
                data_source = announcement.get('data_source', 'api_data')
                if data_source == 'user_created' or data_source == 'user_updated':
                    user_created_count += 1
                else:
                    api_data_count += 1
                
                # í…ìŠ¤íŠ¸ ë‚´ìš© êµ¬ì„± (ì„ë² ë”©ì„ ìœ„í•œ í…ìŠ¤íŠ¸)
                text_content = _build_announcement_text(announcement)
                
                if not text_content.strip():
                    skipped_count += 1
                    continue
                
                # ì„ë² ë”© ìƒì„±
                embedding = chatbot.embedding_manager.create_embedding(text_content)
                
                # ë©”íƒ€ë°ì´í„° êµ¬ì„± (ë°ì´í„° ì†ŒìŠ¤ ì •ë³´ í¬í•¨)
                metadata = _build_announcement_metadata(announcement)
                metadata['data_source'] = data_source  # ë°ì´í„° ì†ŒìŠ¤ ì •ë³´ ì¶”ê°€
                
                # ë²¡í„° ID ìƒì„± (ê³ ìœ í•œ ID)
                vector_id = f"announcement_{announcement_id}"
                
                # ë²¡í„° ë°ì´í„° êµ¬ì„±
                vector_data = {
                    "id": vector_id,
                    "values": embedding,
                    "metadata": metadata
                }
                
                vectors_to_upsert.append(vector_data)
                processed_count += 1
                
                # ë°°ì¹˜ í¬ê¸°ì— ë„ë‹¬í•˜ë©´ ì—…ì„œíŠ¸ ì‹¤í–‰
                if len(vectors_to_upsert) >= 100:
                    success = chatbot.pinecone_manager.upsert_vectors(vectors_to_upsert)
                    if not success:
                        logger.error(f"ë°°ì¹˜ ì—…ì„œíŠ¸ ì‹¤íŒ¨ (processed: {processed_count})")
                        return False, f"ë²¡í„° ì—…ì„œíŠ¸ ì‹¤íŒ¨ (ì²˜ë¦¬ëœ ë°ì´í„°: {processed_count}ê°œ)"
                    
                    vectors_to_upsert.clear()
                    logger.info(f"ğŸ“Š ì§„í–‰ìƒí™©: {processed_count}ê°œ ì²˜ë¦¬ ì™„ë£Œ (ì‚¬ìš©ì: {user_created_count}, API: {api_data_count})")
                
            except Exception as e:
                logger.error(f"ê³µê³  {announcement_id} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                skipped_count += 1
                continue
        
        # ë‚¨ì€ ë²¡í„°ë“¤ ì—…ì„œíŠ¸
        if vectors_to_upsert:
            success = chatbot.pinecone_manager.upsert_vectors(vectors_to_upsert)
            if not success:
                logger.error("ë§ˆì§€ë§‰ ë°°ì¹˜ ì—…ì„œíŠ¸ ì‹¤íŒ¨")
                return False, f"ë§ˆì§€ë§‰ ë°°ì¹˜ ì—…ì„œíŠ¸ ì‹¤íŒ¨ (ì²˜ë¦¬ëœ ë°ì´í„°: {processed_count}ê°œ)"
        
        message = (f"ğŸ‰ í†µí•© Pinecone ì €ì¥ ì™„ë£Œ: {processed_count}ê°œ ì €ì¥ "
                  f"(ì‚¬ìš©ì ìƒì„±: {user_created_count}ê°œ, API ë°ì´í„°: {api_data_count}ê°œ, ìŠ¤í‚µ: {skipped_count}ê°œ)")
        logger.info(message)
        
        return True, message
        
    except Exception as e:
        error_msg = f"í†µí•© Pinecone ë°ì´í„° ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
        logger.error(error_msg)
        return False, error_msg

def _build_announcement_text(announcement: Dict[str, Any]) -> str:
    """
    ê³µê³  ë°ì´í„°ë¥¼ ì„ë² ë”©ì„ ìœ„í•œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ëª¨ë“  ë©”íƒ€ë°ì´í„° í¬í•¨)
    
    Args:
        announcement: ê³µê³  ë°ì´í„°
        
    Returns:
        str: ì„ë² ë”©ìš© í…ìŠ¤íŠ¸ (ëª¨ë“  ë©”íƒ€ë°ì´í„° í¬í•¨)
    """
    # ëª¨ë“  í•„ë“œë¥¼ í¬í•¨í•œ í…ìŠ¤íŠ¸ êµ¬ì„±
    text_parts = []
    
    # 1. í•µì‹¬ ì •ë³´ (ê°€ì¤‘ì¹˜ ë†’ìŒ)
    title = announcement.get('title', '')
    if title:
        text_parts.append(f"ì œëª©: {title}")
        # ì œëª©ì€ ì¤‘ìš”í•˜ë¯€ë¡œ 2ë²ˆ ë°˜ë³µí•˜ì—¬ ê°€ì¤‘ì¹˜ ì¦ê°€
        text_parts.append(f"ì§€ì›ì‚¬ì—…ëª…: {title}")
    
    # 2. ê¸°ê´€ ì •ë³´
    org_name = announcement.get('org_name_ref', announcement.get('organization', ''))
    if org_name:
        text_parts.append(f"ì£¼ê´€ê¸°ê´€: {org_name}")
        text_parts.append(f"ê¸°ê´€ëª…: {org_name}")
    
    # 3. ë¶„ì•¼ ë° ì¹´í…Œê³ ë¦¬ ì •ë³´
    support_field = announcement.get('support_field', announcement.get('category', ''))
    if support_field:
        text_parts.append(f"ì§€ì›ë¶„ì•¼: {support_field}")
        text_parts.append(f"ì¹´í…Œê³ ë¦¬: {support_field}")
    
    # 4. ëŒ€ìƒ ì •ë³´ (ìƒì„¸í•˜ê²Œ)
    target_audience = announcement.get('target_audience', '')
    if target_audience:
        text_parts.append(f"ì‹ ì²­ëŒ€ìƒ: {target_audience}")
        text_parts.append(f"ì§€ì›ëŒ€ìƒ: {target_audience}")
    
    # ì—°ë ¹ëŒ€
    target_age = announcement.get('target_age', '')
    if target_age:
        text_parts.append(f"ì—°ë ¹ëŒ€: {target_age}")
        text_parts.append(f"ë‚˜ì´ì œí•œ: {target_age}")
    
    # ì°½ì—…ê²½í—˜
    startup_experience = announcement.get('startup_experience', '')
    if startup_experience:
        text_parts.append(f"ì°½ì—…ê²½í—˜: {startup_experience}")
        text_parts.append(f"ì‚¬ì—…ê²½ë ¥: {startup_experience}")
    
    # 5. ì§€ì—­ ì •ë³´
    region = announcement.get('region', '')
    if region:
        text_parts.append(f"ì§€ì—­: {region}")
        text_parts.append(f"ì‹ ì²­ì§€ì—­: {region}")
        text_parts.append(f"ì†Œì¬ì§€: {region}")
    
    # 6. ê¸ˆì•¡ ì •ë³´ (ìµœìš°ì„  ì²˜ë¦¬)
    support_content = announcement.get('support_content', '')
    description = announcement.get('description', '')
    
    # ì§€ì›ë‚´ìš©ê³¼ ì„¤ëª…ì—ì„œ ê¸ˆì•¡ ì •ë³´ ì¶”ì¶œ
    all_amounts = []
    if support_content:
        support_amounts = _extract_key_amounts(support_content)
        all_amounts.extend(support_amounts)
    
    if description:
        desc_amounts = _extract_key_amounts(description)
        all_amounts.extend(desc_amounts)
    
    # ì¤‘ë³µ ì œê±° ë° ê¸ˆì•¡ ì •ë³´ ê°•ì¡°
    unique_amounts = list(set(all_amounts))
    if unique_amounts:
        amounts_text = ', '.join(unique_amounts)
        text_parts.append(f"ì§€ì›ê¸ˆì•¡: {amounts_text}")
        text_parts.append(f"ì§€ì›ê·œëª¨: {amounts_text}")
        text_parts.append(f"ì˜ˆì‚°: {amounts_text}")
        # ê¸ˆì•¡ì´ í° ê²½ìš° ì¶”ê°€ ê°•ì¡°
        for amount in unique_amounts:
            if any(keyword in amount for keyword in ['ì–µ', 'ì¡°', 'ì²œë§Œ']):
                text_parts.append(f"ëŒ€ê·œëª¨ì§€ì›: {amount}")
    
    # 7. ì§€ì›ë‚´ìš© ìƒì„¸ (ê¸¸ì´ ì œí•œ ì™„í™”)
    if support_content:
        # ê¸¸ì´ ì œí•œì„ 2000ìë¡œ ì¦ê°€
        if len(support_content) > 2000:
            support_content_short = support_content[:2000]
            # ë¬¸ì¥ì´ ëŠì–´ì§€ì§€ ì•Šë„ë¡ ë§ˆì§€ë§‰ ì™„ì „í•œ ë¬¸ì¥ê¹Œì§€ë§Œ í¬í•¨
            last_period = support_content_short.rfind('.')
            last_newline = support_content_short.rfind('\n')
            cut_point = max(last_period, last_newline)
            if cut_point > 1500:  # ë„ˆë¬´ ì§§ì•„ì§€ì§€ ì•Šë„ë¡
                support_content_short = support_content_short[:cut_point + 1]
        else:
            support_content_short = support_content
        
        text_parts.append(f"ì§€ì›ë‚´ìš©: {support_content_short}")
        text_parts.append(f"ì‚¬ì—…ë‚´ìš©: {support_content_short}")
    
    # 8. ìƒì„¸ ì„¤ëª…
    if description:
        # ì„¤ëª… í…ìŠ¤íŠ¸ (ê¸¸ì´ ì œí•œì„ 1500ìë¡œ ì¦ê°€)
        if len(description) > 1500:
            description_short = description[:1500]
            # ë¬¸ì¥ì´ ëŠì–´ì§€ì§€ ì•Šë„ë¡ ì²˜ë¦¬
            last_period = description_short.rfind('.')
            last_newline = description_short.rfind('\n')
            cut_point = max(last_period, last_newline)
            if cut_point > 1000:
                description_short = description_short[:cut_point + 1]
        else:
            description_short = description
        
        text_parts.append(f"ìƒì„¸ì„¤ëª…: {description_short}")
        text_parts.append(f"ì‚¬ì—…ì„¤ëª…: {description_short}")
    
    # 9. ì¼ì • ì •ë³´
    application_period = announcement.get('application_period', '')
    if application_period:
        text_parts.append(f"ì ‘ìˆ˜ê¸°ê°„: {application_period}")
        text_parts.append(f"ì‹ ì²­ê¸°ê°„: {application_period}")
    
    # ë§ˆê°ì¼ (ì¶”ì¶œëœ ë§ˆê°ì¼)
    deadline = announcement.get('deadline', '')
    if deadline:
        text_parts.append(f"ë§ˆê°ì¼: {deadline}")
        text_parts.append(f"ì¢…ë£Œì¼: {deadline}")
    
    # ê³µê³ ì¼ì
    announcement_date = announcement.get('announcement_date', '')
    if announcement_date:
        text_parts.append(f"ê³µê³ ì¼: {announcement_date}")
        text_parts.append(f"ë°œí‘œì¼: {announcement_date}")
    
    # 10. ì‹ ì²­ ê´€ë ¨ ì •ë³´
    application_method = announcement.get('application_method', [])
    if application_method and isinstance(application_method, list):
        methods = [method for method in application_method if method and 'None' not in method]
        if methods:
            methods_text = ', '.join(methods[:5])  # ìµœëŒ€ 5ê°œ
            text_parts.append(f"ì‹ ì²­ë°©ë²•: {methods_text}")
            text_parts.append(f"ì ‘ìˆ˜ë°©ë²•: {methods_text}")
    
    # 11. ì—°ë½ì²˜ ì •ë³´
    contact = announcement.get('contact', announcement.get('inquiry', ''))
    if contact and contact != 'N/A':
        text_parts.append(f"ì—°ë½ì²˜: {contact}")
        text_parts.append(f"ë¬¸ì˜ì²˜: {contact}")
    
    # 12. ë¶€ì„œ ì •ë³´
    department = announcement.get('department', '')
    if department:
        text_parts.append(f"ë‹´ë‹¹ë¶€ì„œ: {department}")
        text_parts.append(f"ì£¼ê´€ë¶€ì„œ: {department}")
    
    # 13. ì¶”ê°€ ë©”íƒ€ë°ì´í„°
    # ê³µê³  ID
    pblancId = announcement.get('pblancId', '')
    if pblancId:
        text_parts.append(f"ê³µê³ ë²ˆí˜¸: {pblancId}")
    
    # ì‚¬ì—… ìœ í˜•
    business_type = announcement.get('business_type', '')
    if business_type:
        text_parts.append(f"ì‚¬ì—…ìœ í˜•: {business_type}")
    
    # ì§€ì› í˜•íƒœ
    support_type = announcement.get('support_type', '')
    if support_type:
        text_parts.append(f"ì§€ì›í˜•íƒœ: {support_type}")
    
    # 14. í‚¤ì›Œë“œ ì¶”ì¶œ ë° ì¶”ê°€ (ê²€ìƒ‰ ì„±ëŠ¥ í–¥ìƒ)
    all_text = ' '.join(text_parts)
    
    # ìì£¼ ê²€ìƒ‰ë˜ëŠ” í‚¤ì›Œë“œë“¤ ì¶”ê°€
    common_keywords = [
        'ìŠ¤íƒ€íŠ¸ì—…', 'ì°½ì—…', 'ë²¤ì²˜', 'ì¤‘ì†Œê¸°ì—…', 'ì†Œìƒê³µì¸', 
        'AI', 'ì¸ê³µì§€ëŠ¥', 'ë¹…ë°ì´í„°', 'IoT', 'ë¸”ë¡ì²´ì¸',
        'ë°”ì´ì˜¤', 'í—¬ìŠ¤ì¼€ì–´', 'í•€í…Œí¬', 'ì—ë“€í…Œí¬', 'í‘¸ë“œí…Œí¬',
        'ì„œìš¸', 'ë¶€ì‚°', 'ëŒ€êµ¬', 'ì¸ì²œ', 'ê´‘ì£¼', 'ëŒ€ì „', 'ìš¸ì‚°',
        'ê²½ê¸°', 'ê°•ì›', 'ì¶©ë¶', 'ì¶©ë‚¨', 'ì „ë¶', 'ì „ë‚¨', 'ê²½ë¶', 'ê²½ë‚¨', 'ì œì£¼',
        'ì˜ˆë¹„ì°½ì—…ì', 'ì´ˆê¸°ì°½ì—…ì', 'ê¸°ì°½ì—…ì', 'ì²­ë…„', 'ì—¬ì„±',
        'íˆ¬ì', 'ìœµì', 'ë³´ì¡°ê¸ˆ', 'ì§€ì›ê¸ˆ', 'í€ë”©'
    ]
    
    for keyword in common_keywords:
        if keyword in all_text:
            text_parts.append(f"í‚¤ì›Œë“œ: {keyword}")
    
    # 15. ìµœì¢… í…ìŠ¤íŠ¸ êµ¬ì„± (êµ¬ë¶„ìë¡œ ì—°ê²°)
    final_text = " | ".join(text_parts)
    
    # 16. í…ìŠ¤íŠ¸ ê¸¸ì´ ìµœì í™” (ì„ë² ë”© ëª¨ë¸ í† í° ì œí•œ ê³ ë ¤)
    # ëŒ€ë¶€ë¶„ì˜ ì„ë² ë”© ëª¨ë¸ì€ 512 í† í° ì œí•œì´ ìˆìœ¼ë¯€ë¡œ, ì•½ 2000-3000ì ì •ë„ë¡œ ì œí•œ
    if len(final_text) > 3000:
        # ì¤‘ìš”í•œ ì •ë³´ë¶€í„° ìš°ì„  í¬í•¨
        important_parts = []
        remaining_length = 3000
        
        for part in text_parts:
            if len(part) <= remaining_length:
                important_parts.append(part)
                remaining_length -= len(part) + 3  # êµ¬ë¶„ì ê¸¸ì´ ê³ ë ¤
            else:
                break
        
        final_text = " | ".join(important_parts)
    
    return final_text

def _build_announcement_metadata(announcement: Dict[str, Any]) -> Dict[str, Any]:
    """
    ê³µê³  ë°ì´í„°ë¥¼ ë©”íƒ€ë°ì´í„°ë¡œ ë³€í™˜ (ëª¨ë“  ì •ë³´ í¬í•¨)
    
    Args:
        announcement: ê³µê³  ë°ì´í„°
        
    Returns:
        Dict[str, Any]: í™•ì¥ëœ ë©”íƒ€ë°ì´í„°
    """
    # ì„¤ëª…ê³¼ ì§€ì›ë‚´ìš© ê¸¸ì´ ì œí•œ (Pinecone ë©”íƒ€ë°ì´í„° í¬ê¸° ì œí•œ ê³ ë ¤)
    description = announcement.get('description', '')
    if description and len(description) > 2000:  # 1500ì—ì„œ 2000ìœ¼ë¡œ ì¦ê°€
        description = description[:2000] + "..."
    
    support_content = announcement.get('support_content', '')
    if support_content and len(support_content) > 3000:  # 2000ì—ì„œ 3000ìœ¼ë¡œ ì¦ê°€
        support_content = support_content[:3000] + "..."
    
    # ê¸ˆì•¡ ì •ë³´ ì •ê·œí™” (ì§€ì›ë‚´ìš©ê³¼ ì„¤ëª… ëª¨ë‘ì—ì„œ) - ê°œì„ ëœ ë²„ì „ ì‚¬ìš©
    support_amount_info = _normalize_amount(support_content)
    desc_amount_info = _normalize_amount(description)
    title_amount_info = _normalize_amount(announcement.get('title', ''))
    
    # ëª¨ë“  ê¸ˆì•¡ ì •ë³´ ì¤‘ ê°€ì¥ í° ê¸ˆì•¡ì„ ì£¼ ê¸ˆì•¡ìœ¼ë¡œ ì„¤ì •
    all_amount_infos = [support_amount_info, desc_amount_info, title_amount_info]
    main_amount_info = max(all_amount_infos, key=lambda x: x["amount_value"])
    
    # ëª¨ë“  ê¸ˆì•¡ ì •ë³´ ìˆ˜ì§‘ (ê°œì„ ëœ ë²„ì „)
    all_amounts = []
    all_amount_details = []
    
    for amount_info in all_amount_infos:
        if amount_info["amount_value"] > 0:
            all_amounts.append(amount_info["normalized_text"])
            all_amount_details.extend(amount_info["all_amounts"])
    
    # ì¤‘ë³µ ì œê±°
    unique_amounts = list(set(all_amounts))
    
    # ê¸ˆì•¡ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ (ê°œì„ ëœ ë²„ì „)
    amount_category = "ì—†ìŒ"
    has_large_amount = 0.0
    amount_type = main_amount_info.get("amount_type", "ì •í™•")
    
    if main_amount_info["amount_value"] > 0:
        amount_category = _categorize_amount(main_amount_info["amount_value"])
        # ëŒ€ê·œëª¨ ì§€ì›ì‚¬ì—… íŒë³„ (1000ì–µì› ì´ìƒ)
        if main_amount_info["amount_value"] >= 100000000000:  # 1000ì–µì›
            has_large_amount = 1.0
        elif main_amount_info["amount_value"] >= 10000000000:  # 100ì–µì›
            has_large_amount = 0.8
        elif main_amount_info["amount_value"] >= 1000000000:  # 10ì–µì›
            has_large_amount = 0.6
        elif main_amount_info["amount_value"] >= 100000000:  # 1ì–µì›
            has_large_amount = 0.4
        elif main_amount_info["amount_value"] >= 10000000:  # 1ì²œë§Œì›
            has_large_amount = 0.2
    
    # ì‹ ì²­ë°©ë²• ì²˜ë¦¬ (ë¦¬ìŠ¤íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜)
    application_method = announcement.get('application_method', [])
    if isinstance(application_method, list):
        # Noneì´ ì•„ë‹Œ ìœ íš¨í•œ ë°©ë²•ë“¤ë§Œ í•„í„°ë§
        valid_methods = [method for method in application_method if method and 'None' not in method]
        application_method_str = ' | '.join(valid_methods) if valid_methods else 'ì •ë³´ ì—†ìŒ'
    else:
        application_method_str = str(application_method) if application_method else 'ì •ë³´ ì—†ìŒ'
    
    # ì²¨ë¶€íŒŒì¼ ì²˜ë¦¬
    attachments = announcement.get('attachments', [])
    attachments_str = str(len(attachments)) + 'ê°œ' if attachments else 'ì—†ìŒ'
    
    # í‚¤ì›Œë“œ ì¶”ì¶œ
    all_text = f"{announcement.get('title', '')} {support_content} {description}"
    extracted_keywords = []
    
    # ê¸°ìˆ  í‚¤ì›Œë“œ
    tech_keywords = ['AI', 'ì¸ê³µì§€ëŠ¥', 'ë¹…ë°ì´í„°', 'IoT', 'ë¸”ë¡ì²´ì¸', 'ë°”ì´ì˜¤', 'í—¬ìŠ¤ì¼€ì–´', 'í•€í…Œí¬', 'ì—ë“€í…Œí¬', 'í‘¸ë“œí…Œí¬']
    for keyword in tech_keywords:
        if keyword in all_text:
            extracted_keywords.append(keyword)
    
    # ëŒ€ìƒ í‚¤ì›Œë“œ
    target_keywords = ['ìŠ¤íƒ€íŠ¸ì—…', 'ì°½ì—…', 'ë²¤ì²˜', 'ì¤‘ì†Œê¸°ì—…', 'ì†Œìƒê³µì¸', 'ì˜ˆë¹„ì°½ì—…ì', 'ì´ˆê¸°ì°½ì—…ì', 'ê¸°ì°½ì—…ì', 'ì²­ë…„', 'ì—¬ì„±']
    for keyword in target_keywords:
        if keyword in all_text:
            extracted_keywords.append(keyword)
    
    # ì§€ì—­ í‚¤ì›Œë“œ
    region_keywords = ['ì„œìš¸', 'ë¶€ì‚°', 'ëŒ€êµ¬', 'ì¸ì²œ', 'ê´‘ì£¼', 'ëŒ€ì „', 'ìš¸ì‚°', 'ê²½ê¸°', 'ê°•ì›', 'ì¶©ë¶', 'ì¶©ë‚¨', 'ì „ë¶', 'ì „ë‚¨', 'ê²½ë¶', 'ê²½ë‚¨', 'ì œì£¼']
    for keyword in region_keywords:
        if keyword in all_text:
            extracted_keywords.append(keyword)
    
    # ì§€ì› ìœ í˜• í‚¤ì›Œë“œ
    support_keywords = ['íˆ¬ì', 'ìœµì', 'ë³´ì¡°ê¸ˆ', 'ì§€ì›ê¸ˆ', 'í€ë”©', 'ë©˜í† ë§', 'ì»¨ì„¤íŒ…', 'êµìœ¡', 'ì¸íë² ì´íŒ…']
    for keyword in support_keywords:
        if keyword in all_text:
            extracted_keywords.append(keyword)
    
    # ëª¨ë“  ë©”íƒ€ë°ì´í„° êµ¬ì„± (í™•ì¥)
    metadata = {
        # 1. ê¸°ë³¸ ì •ë³´
        "title": announcement.get('title', 'ì œëª© ì—†ìŒ'),
        "organization": announcement.get('org_name_ref', announcement.get('organization', 'ê¸°ê´€ ì •ë³´ ì—†ìŒ')),
        "org_id": str(announcement.get('org_id', '')),
        "department": announcement.get('department', ''),
        "pblancId": str(announcement.get('pblancId', '')),
        
        # 2. ë¶„ì•¼ ë° ì¹´í…Œê³ ë¦¬
        "support_field": announcement.get('support_field', announcement.get('category', 'ë¶„ì•¼ ì •ë³´ ì—†ìŒ')),
        "category": announcement.get('category', announcement.get('support_field', '')),
        "business_type": announcement.get('business_type', ''),
        "support_type": announcement.get('support_type', ''),
        
        # 3. ëŒ€ìƒ ì •ë³´
        "target_audience": announcement.get('target_audience', 'ëŒ€ìƒ ì •ë³´ ì—†ìŒ'),
        "target_age": announcement.get('target_age', ''),
        "startup_experience": announcement.get('startup_experience', ''),
        
        # 4. ì§€ì—­ ì •ë³´
        "region": announcement.get('region', 'ì§€ì—­ ì •ë³´ ì—†ìŒ'),
        
        # 5. ì¼ì • ì •ë³´
        "application_period": announcement.get('application_period', 'ì ‘ìˆ˜ê¸°ê°„ ì •ë³´ ì—†ìŒ'),
        "deadline": announcement.get('deadline', ''),  # ì¶”ì¶œëœ ë§ˆê°ì¼
        "announcement_date": announcement.get('announcement_date', ''),
        "announcement_number": str(announcement.get('announcement_number', '')),
        
        # 6. ë‚´ìš© ì •ë³´
        "description": description or "ì„¤ëª… ì—†ìŒ",
        "support_content": support_content or "ì§€ì›ë‚´ìš© ì •ë³´ ì—†ìŒ",
        
        # 7. ê¸ˆì•¡ ì •ë³´ (í™•ì¥)
        "amount_value": main_amount_info["amount_value"],
        "amount_text": main_amount_info["amount_text"],
        "normalized_amount": main_amount_info["normalized_text"],
        "amount_type": amount_type,  # ì •í™•, ìµœëŒ€, ìµœì†Œ, ì•½ ë“±
        "all_amounts": ' | '.join(unique_amounts) if unique_amounts else '',
        "has_large_amount": has_large_amount,
        "amount_category": amount_category,
        
        # ìƒˆë¡œìš´ ê¸ˆì•¡ ê´€ë ¨ í•„ë“œë“¤
        "amount_in_trillion": main_amount_info["amount_value"] / 1000000000000 if main_amount_info["amount_value"] > 0 else 0,  # ì¡° ë‹¨ìœ„
        "amount_in_billion": main_amount_info["amount_value"] / 100000000 if main_amount_info["amount_value"] > 0 else 0,  # ì–µ ë‹¨ìœ„
        "amount_in_million": main_amount_info["amount_value"] / 10000 if main_amount_info["amount_value"] > 0 else 0,  # ë§Œì› ë‹¨ìœ„
        "is_trillion_scale": 1.0 if main_amount_info["amount_value"] >= 1000000000000 else 0.0,  # ì¡°ì› ê·œëª¨
        "is_hundred_billion_scale": 1.0 if main_amount_info["amount_value"] >= 100000000000 else 0.0,  # ì²œì–µì› ê·œëª¨
        "is_ten_billion_scale": 1.0 if main_amount_info["amount_value"] >= 10000000000 else 0.0,  # ë°±ì–µì› ê·œëª¨
        "is_billion_scale": 1.0 if main_amount_info["amount_value"] >= 100000000 else 0.0,  # ì–µì› ê·œëª¨
        "amount_details_count": len(all_amount_details),  # ë°œê²¬ëœ ê¸ˆì•¡ ì •ë³´ ê°œìˆ˜
        "has_modifier": 1.0 if amount_type != "ì •í™•" else 0.0,  # ìˆ˜ì‹ì–´ í¬í•¨ ì—¬ë¶€
        
        # 8. ì‹ ì²­ ê´€ë ¨
        "application_method": application_method_str,
        "submission_documents": announcement.get('submission_documents', 'ì œì¶œì„œë¥˜ ì •ë³´ ì—†ìŒ'),
        "selection_procedure": announcement.get('selection_procedure', ''),
        
        # 9. ì—°ë½ì²˜
        "contact": announcement.get('contact', announcement.get('inquiry', 'ì—°ë½ì²˜ ì •ë³´ ì—†ìŒ')),
        "inquiry": announcement.get('inquiry', ''),
        
        # 10. ì²¨ë¶€íŒŒì¼
        "attachments_count": attachments_str,
        
        # 11. í‚¤ì›Œë“œ (ê²€ìƒ‰ ì„±ëŠ¥ í–¥ìƒ)
        "keywords": ' | '.join(extracted_keywords) if extracted_keywords else '',
        "tech_keywords": ' | '.join([k for k in extracted_keywords if k in tech_keywords]),
        "target_keywords": ' | '.join([k for k in extracted_keywords if k in target_keywords]),
        "region_keywords": ' | '.join([k for k in extracted_keywords if k in region_keywords]),
        "support_keywords": ' | '.join([k for k in extracted_keywords if k in support_keywords]),
        
        # 12. ê²€ìƒ‰ ìµœì í™” í•„ë“œ
        "searchable_text": f"{announcement.get('title', '')} {announcement.get('org_name_ref', '')} {announcement.get('support_field', '')} {announcement.get('target_audience', '')} {announcement.get('region', '')}",
        "full_text_length": len(f"{description} {support_content}"),
        
        # 13. ìƒíƒœ ì •ë³´
        "is_current": 1,  # í˜„ì¬ ìœ íš¨í•œ ê³µê³ 
        "data_quality": _assess_data_quality(announcement),
        
        # 14. ì‹œìŠ¤í…œ ì •ë³´
        "ingested_at": datetime.now().isoformat(),
        "data_source": "k_startup_api",
        "metadata_version": "2.0"  # ë©”íƒ€ë°ì´í„° ë²„ì „
    }
    
    # ë¹ˆ ê°’ë“¤ì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ëŒ€ì²´
    for key, value in metadata.items():
        if value is None or value == '':
            if key in ['title', 'organization', 'support_field', 'target_audience', 'region']:
                metadata[key] = f"{key} ì •ë³´ ì—†ìŒ"
            elif key in ['amount_value', 'has_large_amount', 'is_current', 'data_quality', 'full_text_length']:
                metadata[key] = 0
            else:
                metadata[key] = "ì •ë³´ ì—†ìŒ"
    
    return metadata

def _categorize_amount(amount_value: int) -> str:
    """ê¸ˆì•¡ì„ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜"""
    if amount_value >= 1000000000000:  # 1ì¡° ì´ìƒ
        return "ì´ˆëŒ€í˜•"
    elif amount_value >= 100000000000:  # 1000ì–µ ì´ìƒ
        return "ëŒ€í˜•"
    elif amount_value >= 10000000000:  # 100ì–µ ì´ìƒ
        return "ì¤‘ëŒ€í˜•"
    elif amount_value >= 1000000000:  # 10ì–µ ì´ìƒ
        return "ì¤‘í˜•"
    elif amount_value >= 100000000:  # 1ì–µ ì´ìƒ
        return "ì†Œí˜•"
    elif amount_value > 0:
        return "ì†Œê·œëª¨"
    else:
        return "ë¯¸ì •"

def _assess_data_quality(announcement: Dict[str, Any]) -> int:
    """ë°ì´í„° í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (0-100)"""
    score = 0
    
    # í•„ìˆ˜ í•„ë“œ ì¡´ì¬ ì—¬ë¶€ (ê° 10ì )
    required_fields = ['title', 'org_name_ref', 'support_field', 'target_audience', 'application_period']
    for field in required_fields:
        if announcement.get(field):
            score += 10
    
    # ìƒì„¸ ì •ë³´ ì¡´ì¬ ì—¬ë¶€ (ê° 5ì )
    detail_fields = ['description', 'support_content', 'contact', 'region', 'deadline']
    for field in detail_fields:
        if announcement.get(field):
            score += 5
    
    # ì¶”ê°€ ì •ë³´ ì¡´ì¬ ì—¬ë¶€ (ê° 5ì )
    extra_fields = ['application_method', 'submission_documents', 'attachments']
    for field in extra_fields:
        if announcement.get(field):
            score += 5
    
    return min(score, 100)

def _extract_amount_condition_from_query(query: str) -> Dict[str, Any]:
    """
    ì‚¬ìš©ì ì¿¼ë¦¬ì—ì„œ ê¸ˆì•¡ ì¡°ê±´ì„ ì¶”ì¶œ
    
    Args:
        query: ì‚¬ìš©ì ì§ˆë¬¸
        
    Returns:
        Dict: {
            "min_amount": int,  # ìµœì†Œ ê¸ˆì•¡ (ì› ë‹¨ìœ„)
            "max_amount": int,  # ìµœëŒ€ ê¸ˆì•¡ (ì› ë‹¨ìœ„, 0ì´ë©´ ì œí•œ ì—†ìŒ)
            "condition_text": str  # ì¶”ì¶œëœ ì¡°ê±´ í…ìŠ¤íŠ¸
        }
    """
    if not query:
        return {"min_amount": 0, "max_amount": 0, "condition_text": ""}
    
    query_lower = query.lower()
    
    # ê¸ˆì•¡ ì¡°ê±´ íŒ¨í„´ë“¤
    patterns = [
        # "1000ì–µ ì› ì´ìƒ", "ì²œì–µ ì´ìƒ"
        (r'(\d+)\s*ì²œ?\s*ì–µ\s*ì›?\s*ì´ìƒ', lambda m: int(m.group(1)) * 100000000000),
        (r'ì²œ\s*ì–µ\s*ì›?\s*ì´ìƒ', lambda m: 100000000000),
        (r'(\d{1,3}(?:,\d{3})*)\s*ì–µ\s*ì›?\s*ì´ìƒ', lambda m: int(m.group(1).replace(',', '')) * 100000000),
        
        # "5ì²œë§Œì› ì´ìƒ", "1ì–µì› ì´ìƒ"
        (r'(\d+)\s*ì²œ\s*ë§Œ\s*ì›?\s*ì´ìƒ', lambda m: int(m.group(1)) * 10000000),
        (r'(\d+)\s*ë°±\s*ë§Œ\s*ì›?\s*ì´ìƒ', lambda m: int(m.group(1)) * 1000000),
        (r'(\d{1,3}(?:,\d{3})*)\s*ë§Œ\s*ì›?\s*ì´ìƒ', lambda m: int(m.group(1).replace(',', '')) * 10000),
        
        # "ìµœì†Œ 1000ì–µ", "ìµœì†Œ ì²œì–µ"
        (r'ìµœì†Œ\s*(\d+)\s*ì²œ?\s*ì–µ\s*ì›?', lambda m: int(m.group(1)) * 100000000000),
        (r'ìµœì†Œ\s*ì²œ\s*ì–µ\s*ì›?', lambda m: 100000000000),
        (r'ìµœì†Œ\s*(\d{1,3}(?:,\d{3})*)\s*ì–µ\s*ì›?', lambda m: int(m.group(1).replace(',', '')) * 100000000),
        
        # "1000ì–µ ì´ìƒì˜", "í° ê·œëª¨ì˜"
        (r'(\d+)\s*ì²œ?\s*ì–µ\s*ì´ìƒì˜?', lambda m: int(m.group(1)) * 100000000000),
        (r'ì²œ\s*ì–µ\s*ì´ìƒì˜?', lambda m: 100000000000),
        (r'(\d{1,3}(?:,\d{3})*)\s*ì–µ\s*ì´ìƒì˜?', lambda m: int(m.group(1).replace(',', '')) * 100000000),
    ]
    
    min_amount = 0
    condition_text = ""
    
    # íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ìµœì†Œ ê¸ˆì•¡ ì°¾ê¸°
    for pattern, converter in patterns:
        match = re.search(pattern, query_lower)
        if match:
            try:
                amount = converter(match)
                if amount > min_amount:
                    min_amount = amount
                    condition_text = match.group(0)
            except (ValueError, AttributeError):
                continue
    
    # í° ê·œëª¨, ëŒ€ê·œëª¨ ë“±ì˜ í‚¤ì›Œë“œ ì²˜ë¦¬
    if not min_amount:
        if any(keyword in query_lower for keyword in ['ëŒ€ê·œëª¨', 'í° ê·œëª¨', 'ëŒ€í˜•', 'ëŒ€ê¸°ì—…', 'ìœ ë‹ˆì½˜']):
            min_amount = 50000000000  # 500ì–µ ì´ìƒ
            condition_text = "ëŒ€ê·œëª¨ ì§€ì›ì‚¬ì—…"
        elif any(keyword in query_lower for keyword in ['ì¤‘ê·œëª¨', 'ì¤‘ê°„ ê·œëª¨']):
            min_amount = 10000000000  # 100ì–µ ì´ìƒ
            condition_text = "ì¤‘ê·œëª¨ ì§€ì›ì‚¬ì—…"
    
    return {
        "min_amount": min_amount,
        "max_amount": 0,  # í˜„ì¬ëŠ” ìµœëŒ€ ê¸ˆì•¡ ì œí•œ ì—†ìŒ
        "condition_text": condition_text
    }

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    chatbot = get_rag_chatbot()
    
    # ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
    status = chatbot.get_system_status()
    print("RAG ì‹œìŠ¤í…œ ìƒíƒœ:")
    for key, value in status.items():
        print(f"  {key}: {value}")
    
    # ê°„ë‹¨í•œ ì§ˆë¬¸ í…ŒìŠ¤íŠ¸ (ì‹¤ì œ ë°ì´í„°ê°€ ìˆì„ ë•Œ)
    # test_query = "ìŠ¤íƒ€íŠ¸ì—… ì§€ì›ì‚¬ì—…ì´ ìˆë‚˜ìš”?"
    # response = chatbot.get_response(test_query)
    # print(f"\nì§ˆë¬¸: {test_query}")
    # print(f"ë‹µë³€: {response['answer']}")
    # print(f"ì‹ ë¢°ë„: {response['confidence']}") 